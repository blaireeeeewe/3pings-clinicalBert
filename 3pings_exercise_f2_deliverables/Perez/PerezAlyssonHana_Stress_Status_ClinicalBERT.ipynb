{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 0) Setup (libraries and reproducibility)\n",
        "\n",
        "**Function Description:**\n",
        "This cell initializes the environment by importing all necessary libraries and setting up reproducibility controls. It loads tools for data manipulation, machine learning, and deep learning, then configures random seeds to ensure consistent results across multiple runs.\n",
        "\n",
        "**Syntax Explanation:**\n",
        "The imports follow a logical grouping pattern. Standard Python libraries like `os`, `math`, and `random` come first, followed by numerical computing tools (`numpy`, `pandas`), and finally the deep learning stack (`torch`, `transformers`, `sklearn`). The `AutoTokenizer` and `AutoModelForSequenceClassification` are convenience classes from Hugging Face that automatically detect and load the correct model architecture based on the checkpoint name you provide. The `TrainingArguments` class acts as a container for all training hyperparameters, while `Trainer` wraps the training loop and handles evaluation, logging, and checkpointing automatically. I set the random seed using `random.seed()`, `np.random.seed()`, `torch.manual_seed()`, and `torch.cuda.manual_seed_all()` to control randomness across all libraries. The device detection uses `torch.cuda.is_available()` to check for GPU availability.\n",
        "\n",
        "**Inputs:**\n",
        "This cell takes no external inputs. It operates on the Python environment itself, importing modules that are either built-in or installable via pip. The SEED value (42) is hardcoded as a constant.\n",
        "\n",
        "**Outputs:**\n",
        "You'll see a single print statement showing which device you're using - \"cuda\" if a GPU is available, \"cpu\" otherwise. This confirmation helps you understand whether training will be fast (GPU) or slow (CPU). GPU training can be 10-50x faster than CPU training for transformer models.\n",
        "\n",
        "**Code Flow:**\n",
        "The cell progresses from general to specific imports, starting with basic Python utilities and ending with specialized deep learning components. After imports, it sets reproducibility seeds across all random number generators. Finally, it detects and prints the compute device. This setup happens once at the beginning and affects all subsequent cells.\n",
        "\n",
        "**Comments and Observations:**\n",
        "Reproducibility is important for scientific experiments and debugging. Without setting seeds, you'd get different train/test splits and different model initializations each time you run the notebook, making it impossible to compare results. The seed value 42 is arbitrary but conventional in machine learning tutorials. GPU availability dramatically impacts training time - a full fine-tuning run that takes 3 hours on CPU might finish in 15 minutes on a GPU. If you're running on Google Colab, make sure you've enabled GPU in Runtime > Change runtime type > Hardware accelerator > GPU. The imports might take 10-30 seconds the first time you run them because Colab needs to load the libraries into memory."
      ],
      "metadata": {
        "id": "SZ7IN9Vp9O6r"
      },
      "id": "SZ7IN9Vp9O6r"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOwoPghthZ2K"
      },
      "source": [
        "## 0) Setup (libraries and reproducibility)"
      ],
      "id": "lOwoPghthZ2K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1V0utC5hZ2L",
        "outputId": "e0e1464a-b307-427c-d5ab-9d1e0194bcda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Every import has an explanatory comment.\n",
        "import os                         # file paths and environment checks\n",
        "import math                       # math helpers (may be useful for schedules)\n",
        "import random                     # Python's RNG for reproducibility\n",
        "import numpy as np                # numerical arrays and metrics support\n",
        "import pandas as pd               # data loading and manipulation\n",
        "from pathlib import Path          # convenient and robust path handling\n",
        "\n",
        "# Hugging Face / PyTorch stack (for transformer fine‚Äëtuning)\n",
        "import torch                      # tensor and GPU utilities\n",
        "from datasets import Dataset      # lightweight dataset wrapper around pandas\n",
        "from transformers import (       # core HF components for tokenization and training\n",
        "    AutoTokenizer,               # auto‚Äëloads the right tokenizer for a given model checkpoint\n",
        "    AutoModelForSequenceClassification,  # classification head on top of a transformer\n",
        "    TrainingArguments,           # training hyperparameters container\n",
        "    Trainer                      # training loop helper (handles eval and logging)\n",
        ")\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Make runs reproducible (seed Python, NumPy, and PyTorch)\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Detect device once and print for visibility\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")  # shows 'cuda' when a GPU is available in Colab\n"
      ],
      "id": "z1V0utC5hZ2L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Load Dataset\n",
        "\n",
        "**Function Description:**\n",
        "This cell handles the complete data loading pipeline, from file upload through data cleaning and label encoding. It prompts you to upload a CSV file, validates the required columns, removes any problematic rows, and converts text labels into numerical format that machine learning models can process.\n",
        "\n",
        "**Syntax Explanation:**\n",
        "The `files.upload()` function from `google.colab` opens a browser file picker that lets you select a CSV from your computer. I capture the uploaded file using `list(uploaded.keys())[0]` which grabs the filename from the dictionary returned by the upload function. The `Path` object creates a cross-platform file path that works on Windows, Mac, and Linux. After loading with `pd.read_csv()`, I use `assert` to verify that both 'statement' and 'status' columns exist - if they don't, the code stops with an error message showing which columns are missing. The `dropna()` method removes any rows where either the text or label is missing, and `copy()` creates a new DataFrame to avoid pandas warnings about modifying views. Converting the statement column with `astype(str)` ensures all entries are strings, even if some got parsed as numbers. The `LabelEncoder` from sklearn automatically creates a mapping from unique text labels to integers (0, 1, 2, etc.) using `fit_transform()`. I temporarily store the encoded values in a new column, then replace the original status column and drop the temporary one.\n",
        "\n",
        "**Inputs:**\n",
        "You provide a CSV file through the browser upload dialog. The CSV must contain at least two columns: 'statement' with the text you want to classify (like \"I feel overwhelmed and can't cope\"), and 'status' with the mental health label (like \"Stress\", \"Anxiety\", \"Normal\"). The labels can be text strings or already-encoded numbers. If you have extra columns like 'Unnamed: 0' (a common artifact from saving DataFrames), they won't break anything.\n",
        "\n",
        "**Outputs:**\n",
        "You'll see several outputs: a confirmation message showing the file path, the label encoding map (which number represents which condition), the count of samples per class, and the first three rows of your cleaned dataset. The label encoding map is particularly important because you'll need it later to interpret predictions - if the model predicts \"5\", you need to know that means \"Stress\". The value counts reveal class imbalance, which affects how you should train your model.\n",
        "\n",
        "**Code Flow:**\n",
        "The flow moves through four distinct phases. First, file upload and path resolution. Second, loading and validation (checking for required columns). Third, data cleaning (removing nulls, ensuring correct data types). Fourth, label encoding (converting text to numbers) with the final reassignment of the status column. Each step depends on the previous one succeeding, which is why I use assertions for critical validations.\n",
        "\n",
        "**Comments and Observations:**\n",
        "Class imbalance is probably your biggest challenge here. If you see something like 16,343 Normal samples but only 1,077 Personality Disorder samples, your model will naturally bias toward predicting Normal because it sees that class 15 times more often. This is why I use class weights in later sections. The `LabelEncoder` assigns numbers alphabetically by default, so \"Anxiety\" becomes 0, \"Bipolar\" becomes 1, and so on. This alphabetical ordering doesn't affect model performance but does affect how you read the results. Some datasets have text encoding issues (weird characters, emojis) that can cause problems during tokenization. If you see strange symbols in the data preview, you might need to add encoding='utf-8' or encoding='latin-1' to the `read_csv()` call. The label encoder will fail if your status column has typos (like \"Stres\" vs \"Stress\") because it treats them as different classes. Always check your label counts to catch these issues."
      ],
      "metadata": {
        "id": "EGXgy5Be9SIS"
      },
      "id": "EGXgy5Be9SIS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ3rjAL2hZ2L"
      },
      "source": [
        "## 1) Load Dataset"
      ],
      "id": "wQ3rjAL2hZ2L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "CMBpEom1hZ2M",
        "outputId": "0b419b6c-46d6-4b46-89e7-1b34b216b0b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Please upload your dataset CSV (e.g., Combined Data.csv)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4da332ec-4edb-4bfb-9dc0-c6593407b70f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4da332ec-4edb-4bfb-9dc0-c6593407b70f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Combined Data.csv to Combined Data (2).csv\n",
            "‚úÖ File uploaded successfully: /content/Combined Data (2).csv\n",
            "üî§ Label encoding map:\n",
            "  0 ‚Üí Anxiety\n",
            "  1 ‚Üí Bipolar\n",
            "  2 ‚Üí Depression\n",
            "  3 ‚Üí Normal\n",
            "  4 ‚Üí Personality disorder\n",
            "  5 ‚Üí Stress\n",
            "  6 ‚Üí Suicidal\n",
            "\n",
            "‚úÖ Dataset loaded and label-encoded successfully!\n",
            "status\n",
            "3    16343\n",
            "2    15404\n",
            "6    10652\n",
            "0     3841\n",
            "1     2777\n",
            "5     2587\n",
            "4     1077\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                          statement  status\n",
              "0           0                                         oh my gosh       0\n",
              "1           1  trouble sleeping, confused mind, restless hear...       0\n",
              "2           2  All wrong, back off dear, forward doubt. Stay ...       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57642578-bac5-498e-bc81-5c4d7597b015\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>statement</th>\n",
              "      <th>status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>oh my gosh</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>trouble sleeping, confused mind, restless hear...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>All wrong, back off dear, forward doubt. Stay ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57642578-bac5-498e-bc81-5c4d7597b015')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-57642578-bac5-498e-bc81-5c4d7597b015 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-57642578-bac5-498e-bc81-5c4d7597b015');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f6b88fae-1411-4565-a5ae-000d91305bfd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f6b88fae-1411-4565-a5ae-000d91305bfd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f6b88fae-1411-4565-a5ae-000d91305bfd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 52681,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15235,\n        \"min\": 0,\n        \"max\": 53042,\n        \"num_unique_values\": 52681,\n        \"samples\": [\n          3008,\n          44705,\n          50186\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"statement\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 51073,\n        \"samples\": [\n          \"he's been a chain smoker for 30 years.\",\n          \"Dependence on therapist I attend IOP groups and individual therapy sessions at the same place, my therapist who I have worked with on and off for a year and a couple months just told me today that she is leaving soon and I am heartbroken. I love my therapist and I don't know how I am going to keep progressing without her. There will be a replacement for her but idk what to do, I don't want a different therapist. :(\",\n          \"These feelings constantly come back. Someone from my past that hurt me came back a month ago and once again disrespected me and i just feel like shit. Idk why these feelings keep resurfacing but it just hurts. I do not want to be over dramatic but Its hurts when you were nothing but loving/kind to someone and they disrespect you. I just hate feeling like this, feeling like i cannot trust anyone or that no one would ever truly love me unless i have something to offer. I am always worried about my looks and its just making me depressed. I really do not feel like i fit in with the world I am just here. Idk what my next step should be to get help but I am really going through it. (Yes I am in therapy) but how do i help myself ? I have been depressed/anxious for years and most day i do not even leave my house. But nobody around me seems to care and honestly I am tired of feeling this way. But at the same time i do not want to give up on myself bc i feel like I am here to be somebody great. I am just trying to find my way right now. It keeps coming back\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# --- Load Dataset (Upload version, auto-encodes text labels) ---\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "print(\"üìÇ Please upload your dataset CSV (e.g., Combined Data.csv)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Automatically pick the first uploaded file\n",
        "filename = list(uploaded.keys())[0]\n",
        "csv_path = Path(f\"/content/{filename}\")\n",
        "\n",
        "print(f\"‚úÖ File uploaded successfully: {csv_path}\")\n",
        "\n",
        "# Load the CSV\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# --- Validate columns ---\n",
        "expected_cols = {'statement', 'status'}\n",
        "assert expected_cols.issubset(df.columns), f\"‚ùå Missing required columns: {expected_cols - set(df.columns)}\"\n",
        "\n",
        "# --- Clean ---\n",
        "df = df.dropna(subset=['statement', 'status']).copy()\n",
        "df['statement'] = df['statement'].astype(str)\n",
        "\n",
        "# --- Encode text labels into integers ---\n",
        "# This maps each unique label (like 'Anxiety', 'Stress', etc.) to a numeric ID\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['status_encoded'] = le.fit_transform(df['status'])\n",
        "\n",
        "# Optional: print mapping for your reference\n",
        "print(\"üî§ Label encoding map:\")\n",
        "for label, code in zip(le.classes_, range(len(le.classes_))):\n",
        "    print(f\"  {code} ‚Üí {label}\")\n",
        "\n",
        "# Replace 'status' with the encoded version\n",
        "df['status'] = df['status_encoded']\n",
        "df.drop(columns=['status_encoded'], inplace=True)\n",
        "\n",
        "print(\"\\n‚úÖ Dataset loaded and label-encoded successfully!\")\n",
        "print(df['status'].value_counts(dropna=False))\n",
        "df.head(3)\n"
      ],
      "id": "CMBpEom1hZ2M"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Baseline Models (TF-IDF + Linear)\n",
        "\n",
        "**Function Description:**\n",
        "This cell establishes performance baselines using traditional machine learning before moving to deep learning. It splits your data, converts text to numerical features using TF-IDF, trains two simple linear models (Logistic Regression and Linear SVM), and reports their accuracy, precision, recall, and F1 scores.\n",
        "\n",
        "**Syntax Explanation:**\n",
        "The `train_test_split()` function from sklearn divides your data into 80% training and 20% validation using the random state 42 for reproducibility. The `stratify` parameter ensures both sets maintain the same class distribution as your original data. `TfidfVectorizer` converts text into numbers by analyzing word frequencies - the `ngram_range=(1,2)` parameter means it considers both individual words and two-word phrases, `min_df=2` ignores words appearing in fewer than 2 documents (filtering out typos and rare terms), and `max_features=40000` keeps only the 40,000 most informative features. The vectorizer's `fit_transform()` learns the vocabulary from training data and converts it to features in one step, while `transform()` applies that learned vocabulary to validation data without learning anything new. Both `LogisticRegression` and `LinearSVC` use `class_weight=\"balanced\"` which automatically adjusts for class imbalance by computing weights inversely proportional to class frequencies. The `precision_recall_fscore_support()` function calculates all metrics at once, and the `average` parameter determines how to aggregate across multiple classes (weighted average accounts for class imbalance).\n",
        "\n",
        "**Inputs:**\n",
        "This cell takes the cleaned DataFrame from the previous section and specifically uses the 'statement' column (text) as features and 'status' column (labels) as targets. The `train_test_split()` randomly selects which samples go into training vs validation based on the test_size ratio and random seed.\n",
        "\n",
        "**Outputs:**\n",
        "You get performance metrics for both baseline models printed in a compact format showing accuracy, precision, recall, and F1 score. The cell also prints which averaging method it's using (binary for 2 classes, weighted for more than 2) based on automatic detection of the number of unique classes. Typical baseline scores range from 75-85% accuracy depending on your data quality and class separability.\n",
        "\n",
        "**Code Flow:**\n",
        "The code follows a standard machine learning pipeline. First, split the data to create independent train and test sets. Second, fit the TF-IDF vectorizer on training text and transform both sets. Third, train the first model (Logistic Regression), make predictions, and calculate metrics. Fourth, repeat the training and evaluation process for the second model (Linear SVM). The vectorizer must be fit before the classifiers because the classifiers need fixed-size numerical inputs.\n",
        "\n",
        "**Comments and Observations:**\n",
        "These baseline models serve two purposes: they give you a performance floor that deep learning should beat, and they train in seconds rather than hours, letting you quickly spot data quality issues. If your baseline F1 is below 60%, something's wrong with your data (mislabeled samples, too much noise, or the classes aren't actually distinguishable from text alone). TF-IDF works surprisingly well for text classification because it captures which words are distinctive for each class. For example, the word \"overwhelmed\" might appear frequently in stress-related texts but rarely in normal texts, giving it high TF-IDF weight. The ngram_range=(1,2) parameter helps capture phrases like \"panic attack\" or \"feel good\" that carry more meaning than individual words. Linear models like Logistic Regression are also interpretable - you could examine the feature weights to see which words most strongly predict each class. The max_features limit prevents the feature space from exploding (some datasets have 100k+ unique words) and also acts as regularization by forcing the model to focus on the most informative terms. SVM typically performs slightly better than Logistic Regression on text because it finds the maximum-margin decision boundary, but both usually give similar results. If SVM and Logistic Regression give very different scores (more than 5% gap), that suggests your data has complex class boundaries that might benefit from deep learning."
      ],
      "metadata": {
        "id": "g0kKhn-N9Ydz"
      },
      "id": "g0kKhn-N9Ydz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4IjGl_JhZ2M",
        "outputId": "9664c070-a5a5-4a8b-eed7-3d14618b933f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected 7 classes ‚Üí using average='weighted' for metrics.\n",
            "\n",
            "[Baseline-LR] Acc=0.778  P=0.787  R=0.778  F1=0.777\n",
            "[Baseline-SVM] Acc=0.782  P=0.779  R=0.782  F1=0.780\n"
          ]
        }
      ],
      "source": [
        "# --- Baseline Models (TF-IDF + Linear, supports multi-class) ---\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "# Train/validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df['statement'].values,\n",
        "    df['status'].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['status'].values\n",
        ")\n",
        "\n",
        "# Convert raw text into TF-IDF features\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=2, max_features=40000)\n",
        "Xtr = tfidf.fit_transform(X_train)\n",
        "Xva = tfidf.transform(X_val)\n",
        "\n",
        "# Detect if this is binary or multiclass\n",
        "num_classes = len(np.unique(y_train))\n",
        "avg_type = \"binary\" if num_classes == 2 else \"weighted\"\n",
        "print(f\"Detected {num_classes} classes ‚Üí using average='{avg_type}' for metrics.\\n\")\n",
        "\n",
        "# --- Baseline 1: Logistic Regression ---\n",
        "logreg = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
        "logreg.fit(Xtr, y_train)\n",
        "pred_lr = logreg.predict(Xva)\n",
        "p, r, f, _ = precision_recall_fscore_support(y_val, pred_lr, average=avg_type)\n",
        "acc = accuracy_score(y_val, pred_lr)\n",
        "print(f\"[Baseline-LR] Acc={acc:.3f}  P={p:.3f}  R={r:.3f}  F1={f:.3f}\")\n",
        "\n",
        "# --- Baseline 2: Linear SVM ---\n",
        "svm = LinearSVC(class_weight=\"balanced\")\n",
        "svm.fit(Xtr, y_train)\n",
        "pred_svm = svm.predict(Xva)\n",
        "p, r, f, _ = precision_recall_fscore_support(y_val, pred_svm, average=avg_type)\n",
        "acc = accuracy_score(y_val, pred_svm)\n",
        "print(f\"[Baseline-SVM] Acc={acc:.3f}  P={p:.3f}  R={r:.3f}  F1={f:.3f}\")\n"
      ],
      "id": "k4IjGl_JhZ2M"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Pre-Trained Models (Tokenization and Dataset Prep)\n",
        "\n",
        "**Function Description:**\n",
        "This cell prepares your text data for transformer models by loading a specialized tokenizer and converting all text into the numerical format that BERT-based models expect. It tokenizes both training and validation texts, then packages them into HuggingFace Dataset objects that work seamlessly with the Trainer API.\n",
        "\n",
        "**Syntax Explanation:**\n",
        "I define two model checkpoint names as constants - `CLINICAL_BERT` points to a model trained on clinical text, while `DISTIL_BERT` points to a smaller, faster baseline. The `AutoTokenizer.from_pretrained()` method downloads and initializes the tokenizer that matches your chosen model architecture. The `tokenize_texts()` helper function takes a list of strings and converts them to token IDs - the `padding=True` parameter adds zeros to shorter sequences so all sequences in a batch have the same length, `truncation=True` cuts off text exceeding the max_length, `max_length=128` sets the sequence limit, and `return_tensors=\"pt\"` formats the output as PyTorch tensors rather than lists. After tokenizing, I use `Dataset.from_dict()` to create HuggingFace datasets, passing dictionaries that contain input_ids (the tokenized text), attention_mask (which positions are real tokens vs padding), and labels (your encoded status values wrapped in `torch.tensor()`).\n",
        "\n",
        "**Inputs:**\n",
        "This cell uses the `X_train`, `X_val`, `y_train`, and `y_val` arrays created by the train_test_split in the previous section. X_train and X_val contain the text statements, while y_train and y_val contain the corresponding numerical labels.\n",
        "\n",
        "**Outputs:**\n",
        "You'll see progress bars as the tokenizer downloads (first run only), then the final line shows the sizes of your train and validation datasets as a tuple like (42144, 10537). This confirms you have roughly 80% of samples in training and 20% in validation. The tokenized datasets are stored in memory as `train_ds` and `val_ds` objects ready for training.\n",
        "\n",
        "**Code Flow:**\n",
        "The flow is straightforward and sequential. First, I define model checkpoints and select one as the default backbone. Second, I load the tokenizer for that backbone. Third, I define a helper function that wraps the tokenizer with specific parameters. Fourth, I apply that function to both train and validation texts. Fifth, I package the tokenized outputs and labels into Dataset objects. This preparation step only happens once before training multiple experiments.\n",
        "\n",
        "**Comments and Observations:**\n",
        "The choice between ClinicalBERT and DistilBERT matters more than you might think. ClinicalBERT was trained on clinical notes, discharge summaries, and medical text, so it understands medical terminology and the way healthcare professionals write. This makes it better suited for mental health classification where text might include clinical terms or formal descriptions. DistilBERT is a compressed version of BERT with 40% fewer parameters - it trains faster and uses less memory but might miss subtle patterns that the full model catches. The max_length=128 setting is a practical choice that balances speed and information retention. Most mental health statements are 20-80 words, which translates to roughly 30-120 tokens after subword tokenization. Setting max_length too high wastes computation on padding, while setting it too low truncates important information. The tokenizer uses subword tokenization, meaning it breaks rare or complex words into pieces - for example, \"unhappiness\" might become [\"un\", \"happiness\"]. This helps the model handle words it's never seen before by understanding their components. The attention_mask is important because it tells the model which tokens are real (value of 1) and which are padding (value of 0), preventing padding tokens from influencing the model's predictions. When you see the tokenizer downloading files, it's fetching the vocabulary file (which maps words to IDs) and the config file (which stores tokenization parameters). These downloads only happen once and get cached locally."
      ],
      "metadata": {
        "id": "fSUPDc6_-7JH"
      },
      "id": "fSUPDc6_-7JH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xic-iii4hZ2N"
      },
      "source": [
        "## 3) Pre‚ÄëTrained Models (Tokenization and Dataset Prep)"
      ],
      "id": "xic-iii4hZ2N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222,
          "referenced_widgets": [
            "591cdd45c7b74fa181821f9529944da2",
            "776524b1ce5c4c5bb65f8d8e65516273",
            "3d089afa63744558bb3a6f7a1b876aef",
            "e4a09e0a755241daad5e0330cfe68fd2",
            "b81c22472d7940c798254fd44d9877b8",
            "5da86772c65244f09c694822bd730ce2",
            "1cd05c5690a743bbb2472e7b3d8a91c5",
            "b07464960b8645bbaca506d2ef31bace",
            "5fde31686a35414f962235ea930bdccb",
            "4b95ecc756ea41f29095ab7890ab856c",
            "5a1b98fe2c514b45a8153a30ba29225b",
            "bd7b191158e14b56b7fb18d781905220",
            "c86017ab938b4f46baa78b381b03320c",
            "11930ebd9ce2434c8aaf21655a0f564a",
            "e217ae1ba3974278934fbb75fae99d57",
            "3c9e5917c4754acbbb908f3431191d45",
            "1c6926731b6e4c06a63e5a2ef414f6a9",
            "35c4939c98a141cdaecbbd978211e48d",
            "ddd174a933954b4fab9f1640ad491973",
            "ade9e93734a1456797e0340e38182c04",
            "74b176f12a3d42139088e6f4dd3f2d12",
            "08e6b823fa1042c9892bc49c949a2857"
          ]
        },
        "id": "CJsRDxIlhZ2N",
        "outputId": "baf876dd-c9d2-4b5e-8220-70328e404cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "591cdd45c7b74fa181821f9529944da2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd7b191158e14b56b7fb18d781905220"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42144, 10537)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "\n",
        "# Choose your checkpoints.\n",
        "# We include ClinicalBERT (for clinical text) and DistilBERT (fast baseline).\n",
        "CLINICAL_BERT = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "DISTIL_BERT   = \"distilbert-base-uncased\"\n",
        "\n",
        "# Pick one as the default backbone for experiments below.\n",
        "BACKBONE = CLINICAL_BERT\n",
        "\n",
        "# Initialize tokenizer for the chosen backbone\n",
        "tokenizer = AutoTokenizer.from_pretrained(BACKBONE)\n",
        "\n",
        "# Helper to tokenize a pandas series with per-line comments\n",
        "def tokenize_texts(texts, max_length=128):\n",
        "    # Apply the tokenizer: returns dict with input_ids and attention_mask\n",
        "    return tokenizer(\n",
        "        list(texts),                 # a Python list of strings\n",
        "        padding=True,                # pad to the longest in the batch\n",
        "        truncation=True,             # cut off text exceeding max_length\n",
        "        max_length=max_length,       # cap sequence length\n",
        "        return_tensors=\"pt\"          # return PyTorch tensors\n",
        "    )\n",
        "\n",
        "# Tokenize train/validation splits\n",
        "train_enc = tokenize_texts(X_train)\n",
        "val_enc   = tokenize_texts(X_val)\n",
        "\n",
        "# Wrap into HF Datasets with labels\n",
        "train_ds = Dataset.from_dict({\n",
        "    \"input_ids\": train_enc[\"input_ids\"],\n",
        "    \"attention_mask\": train_enc[\"attention_mask\"],\n",
        "    \"labels\": torch.tensor(y_train)\n",
        "})\n",
        "val_ds = Dataset.from_dict({\n",
        "    \"input_ids\": val_enc[\"input_ids\"],\n",
        "    \"attention_mask\": val_enc[\"attention_mask\"],\n",
        "    \"labels\": torch.tensor(y_val)\n",
        "})\n",
        "\n",
        "len(train_ds), len(val_ds)\n"
      ],
      "id": "CJsRDxIlhZ2N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Training of Data (Trainer utilities and metrics)\n",
        "\n",
        "**Function Description:**\n",
        "This cell sets up the infrastructure you need for training - specifically the metric computation function and the custom weighted loss. It defines how to evaluate model performance and how to handle class imbalance during training by penalizing mistakes on rare classes more heavily.\n",
        "\n",
        "**Syntax Explanation:**\n",
        "The `compute_metrics()` function takes an `eval_pred` tuple containing logits (raw model outputs before softmax) and true labels. Inside the function, `np.argmax(logits, axis=-1)` converts logits to class predictions by selecting the highest value along the last dimension. The `precision_recall_fscore_support()` function calculates all four metrics in one call using the average parameter to specify how to aggregate across classes (binary for 2-class, weighted for multi-class). For class weights, I count how many samples exist in each class using `(y_train == 1).sum()` for the positive class and similar for negative, then compute the weight for the positive class as `neg / max(pos, 1)` which gives higher weight to the minority class. The `max(pos, 1)` prevents division by zero if you somehow have zero positive samples. I create a PyTorch tensor from these weights and move it to the correct device using `.to(device)`. The `WeightedTrainer` class inherits from HuggingFace's `Trainer` and overrides only the `compute_loss()` method. Inside that method, I extract labels from inputs, run the model on the remaining inputs (everything except labels), get the logits from outputs, create a CrossEntropyLoss function with the class weights, and calculate loss by comparing predictions to true labels.\n",
        "\n",
        "**Inputs:**\n",
        "This cell uses `y_train` from the earlier train-test split to compute class frequencies and create weights. The compute_metrics function receives predictions from the Trainer during evaluation, while the WeightedTrainer receives model inputs, labels, and the model itself during training.\n",
        "\n",
        "**Outputs:**\n",
        "You'll see the class weights printed as a list showing the weight for each class. For a binary case with 20,000 negative and 5,000 positive samples, you'd see weights like [1.0, 4.0], meaning the model pays 4x more attention to positive class errors. For multi-class problems with severe imbalance, some weights might be 10x or higher.\n",
        "\n",
        "**Code Flow:**\n",
        "The code sets up two separate but related pieces of infrastructure. First, it defines and prints class weights that quantify the imbalance in your data. Second, it creates a custom Trainer class that uses those weights during loss calculation. These components work together during training - the Trainer calls compute_loss every batch to calculate weighted loss, and calls compute_metrics every epoch to evaluate on the validation set.\n",
        "\n",
        "**Comments and Observations:**\n",
        "Class imbalance is one of the biggest challenges in mental health classification. Without weighting, a model trained on data that's 80% Normal and 20% Other could achieve 80% accuracy by always predicting Normal and completely ignoring the minority classes. Weighted loss forces the model to care about all classes by making errors on rare classes expensive. The math behind class weighting is intuitive - if you have 4x more samples of class A than class B, you give class B a weight of 4.0 so that one mistake on class B costs as much as four mistakes on class A. This balances the gradient updates and prevents the model from ignoring minority classes. CrossEntropyLoss is the standard loss function for classification because it measures the difference between predicted probability distributions and true labels. Adding weights modifies the formula to multiply each sample's loss by its class weight before averaging. The custom Trainer override is necessary because the default Trainer doesn't support class weights out of the box. By inheriting and overriding just the compute_loss method, you keep all the other Trainer functionality (logging, checkpointing, evaluation) while adding custom loss calculation. The `return_outputs` parameter in compute_loss determines whether to return just the loss (for training) or both loss and full model outputs (for when you need predictions), and I handle both cases with the conditional return statement. This weighted approach works well for moderate imbalance (ratios up to 10:1 or 20:1) but for extreme imbalance you might need additional techniques like oversampling the minority class or using focal loss."
      ],
      "metadata": {
        "id": "6Es8uR0B--FD"
      },
      "id": "6Es8uR0B--FD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A6SNywxhZ2N"
      },
      "source": [
        "## 4) Training of Data (Trainer utilities and metrics)"
      ],
      "id": "2A6SNywxhZ2N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_4oBKl6hZ2N",
        "outputId": "66d05985-aab7-4927-d23a-7341b0cd56be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights (neg, pos): [1.0, 1.3836109638214111]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Metric function for the Trainer: computes Accuracy, Precision, Recall, F1\n",
        "def compute_metrics(eval_pred):\n",
        "    # eval_pred is a tuple of (logits, labels)\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# Optional: class weights for imbalanced datasets\n",
        "# Compute weights inversely proportional to class frequencies\n",
        "pos = (y_train == 1).sum()\n",
        "neg = (y_train == 0).sum()\n",
        "w_pos = neg / max(pos, 1)   # weight for positive class\n",
        "w_neg = 1.0                 # keep negative as baseline\n",
        "class_weights = torch.tensor([w_neg, w_pos], dtype=torch.float).to(device)\n",
        "print(f\"Class weights (neg, pos): {class_weights.tolist()}\" )\n",
        "\n",
        "# Custom Trainer that injects weighted loss\n",
        "from torch.nn import CrossEntropyLoss\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = CrossEntropyLoss(weight=class_weights)\n",
        "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ],
      "id": "9_4oBKl6hZ2N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Fine-tuning (Three Experiments)\n",
        "\n",
        "**Function Description:**\n",
        "This cell runs three complete fine-tuning experiments with different hyperparameter configurations. It trains transformer models on your data, evaluates them on the validation set, and creates a leaderboard ranking them by F1 score. Each experiment uses different settings for learning rate, batch size, epochs, and model architecture to find the best configuration for your specific dataset.\n",
        "\n",
        "**Syntax Explanation:**\n",
        "The code starts by detecting the number of unique classes with `len(np.unique(y_train))` and setting the averaging strategy for metrics accordingly. The `compute_metrics()` function here is similar to Section 4 but adapts to multi-class by using weighted averaging. For class weights, I use `np.bincount(y_train, minlength=num_labels)` which counts occurrences of each class, then compute weights as `counts.max() / np.maximum(counts, 1)` which gives higher weights to rarer classes while avoiding division by zero. The weights become a PyTorch tensor on the correct device. The `WeightedTrainer` class override works identically to Section 4 but now handles the multi-class case properly. The `tokenize_texts()` function accepts a max_length parameter to allow different experiments to use different sequence lengths. The `make_training_args()` function is a factory that creates `TrainingArguments` objects with version compatibility - it first tries the modern signature with `evaluation_strategy` and `save_strategy`, and if that fails (older transformers versions), it falls back to legacy parameters like `do_eval`. The `run_experiment()` function orchestrates everything: it re-tokenizes data with the specified max_length, creates fresh Dataset objects, loads the pre-trained model with `AutoModelForSequenceClassification.from_pretrained()` while specifying the correct number of output classes, creates training arguments, instantiates the WeightedTrainer, calls `trainer.train()` to run training, calls `trainer.evaluate()` to get final metrics, and returns both metrics and the trainer object. Each of the three experiments (A, B, C) calls `run_experiment()` with different parameters and stores results in an OrderedDict. Finally, I extract the F1 scores from each result, sort experiments by F1, and print a leaderboard.\n",
        "\n",
        "**Inputs:**\n",
        "This cell uses `X_train`, `X_val`, `y_train`, and `y_val` from the train-test split. It also uses the model checkpoint names (CLINICAL_BERT, DISTIL_BERT) and the tokenizer defined in earlier sections. Each experiment re-tokenizes the data with its specific max_length setting.\n",
        "\n",
        "**Outputs:**\n",
        "During training, you'll see progress bars showing epoch progress, batch progress within each epoch, loss values that should decrease over time, and periodic evaluation metrics. After each experiment finishes, you'll see a summary of its final performance metrics including accuracy, precision, recall, and F1 score. At the very end, a leaderboard ranks all three experiments by F1 score, showing which configuration worked best. The entire cell might take 20-60 minutes to run depending on whether you have GPU and how large your dataset is.\n",
        "\n",
        "**Code Flow:**\n",
        "The flow is hierarchical and modular. At the top level, I set up shared infrastructure (metrics function, class weights). Then I define helper functions (tokenization, training args factory, experiment runner). Finally, I call the experiment runner three times with different parameters and collect results. Each experiment is independent - they don't share trained weights, though they do share the data and evaluation metrics. The leaderboard aggregation happens after all experiments complete, sorting by F1 score and displaying in descending order.\n",
        "\n",
        "**Comments and Observations:**\n",
        "Hyperparameter selection is part science, part art. Learning rates for fine-tuning transformers typically range from 1e-5 to 5e-5 because these models are already well-trained and you don't want to disturb the pre-trained weights too much. Going higher risks catastrophic forgetting where the model loses its pre-trained knowledge. Batch size is constrained by GPU memory - if you get out-of-memory errors, reduce batch size. Smaller batches (8-16) give noisier gradient updates but sometimes generalize better, while larger batches (32-64) give more stable training but might overfit. The number of epochs depends on dataset size - smaller datasets need more epochs to converge, but too many epochs causes overfitting. Weight decay adds L2 regularization by penalizing large weights, which prevents overfitting but too much weight decay can underfit. Warmup ratio gradually increases the learning rate from near-zero to the target value over the first X% of training steps, which stabilizes training when starting from random initialization of the classification head. The version compatibility fallback exists because HuggingFace frequently changes their API - older transformers versions used different parameter names for the same functionality. By catching the TypeError and falling back to legacy parameters, the code works across a wider range of library versions. Experiment A (conservative) uses safe defaults that should work reliably but might not achieve peak performance. Experiment B (aggressive) pushes the learning rate higher and trains longer, which might find better optima but risks overfitting. Experiment C (fast baseline) uses DistilBERT for speed comparison - if DistilBERT matches ClinicalBERT performance, you might prefer it for production due to faster inference. The leaderboard at the end tells you objectively which approach worked best on your specific data. Sometimes the aggressive approach wins, sometimes the conservative one does - it depends on your data characteristics and how much overfitting risk you face. The F1 score is the key metric here because it balances precision and recall, giving you a single number that captures overall classification quality while accounting for class imbalance through weighted averaging."
      ],
      "metadata": {
        "id": "QzUAOSA6-_fL"
      },
      "id": "QzUAOSA6-_fL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvrjk1PlhZ2N"
      },
      "source": [
        "## 5) Fine‚Äëtuning (Three Experiments)"
      ],
      "id": "Dvrjk1PlhZ2N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "LkYDOan1hZ2O",
        "outputId": "55215eba-a0e6-4915-947f-809800dc0e16"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4064124003.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 1) Metrics: binary vs multiclass handled automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mavg_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"binary\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"weighted\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Fine-tune] Detected {num_labels} classes ‚Üí metrics average='{avg_type}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
          ]
        }
      ],
      "source": [
        "# --- 5) Fine-tuning (Three Experiments) [version-compatible] ---\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1) Metrics: binary vs multiclass handled automatically\n",
        "num_labels = len(np.unique(y_train))\n",
        "avg_type = \"binary\" if num_labels == 2 else \"weighted\"\n",
        "print(f\"[Fine-tune] Detected {num_labels} classes ‚Üí metrics average='{avg_type}'\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "    p, r, f, _ = precision_recall_fscore_support(labels, preds, average=avg_type)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f}\n",
        "\n",
        "# 2) Class weights for imbalanced data (size == num_labels)\n",
        "counts = np.bincount(y_train, minlength=num_labels)\n",
        "# Heuristic: inverse-frequency scaled to max=1.0 (safe for CE)\n",
        "weights = counts.max() / np.maximum(counts, 1)\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32, device=device)\n",
        "print(f\"[Fine-tune] Class weights: {class_weights.tolist()}\")\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = CrossEntropyLoss(weight=class_weights)\n",
        "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# 3) Helper: tokenizer already defined above. Re-tokenize per max_length\n",
        "def tokenize_texts(texts, max_length=160):\n",
        "    return tokenizer(\n",
        "        list(texts),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# 4) Version-compatible TrainingArguments factory\n",
        "import inspect\n",
        "\n",
        "def make_training_args(name, batch_size, lr, epochs, weight_decay, warmup_ratio):\n",
        "    kwargs_modern = dict(\n",
        "        output_dir=f\"./runs/{name}\",\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        learning_rate=lr,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=weight_decay,\n",
        "        warmup_ratio=warmup_ratio,\n",
        "        logging_steps=50,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        report_to=[]\n",
        "    )\n",
        "    try:\n",
        "        # Try modern signature first\n",
        "        return TrainingArguments(**kwargs_modern)\n",
        "    except TypeError:\n",
        "        # Fallback for older transformers (no evaluation_strategy/save_strategy)\n",
        "        print(\"[Fine-tune] Using legacy TrainingArguments fallback.\")\n",
        "        kwargs_legacy = dict(\n",
        "            output_dir=f\"./runs/{name}\",\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            learning_rate=lr,\n",
        "            num_train_epochs=epochs,\n",
        "            weight_decay=weight_decay,\n",
        "            logging_steps=50,\n",
        "            do_eval=True,          # legacy way to enable evaluation\n",
        "            save_steps=500,        # periodic saving\n",
        "            overwrite_output_dir=True,\n",
        "            fp16=torch.cuda.is_available()\n",
        "        )\n",
        "        return TrainingArguments(**kwargs_legacy)\n",
        "\n",
        "def run_experiment(name, backbone, batch_size=16, lr=2e-5, epochs=3,\n",
        "                   weight_decay=0.01, warmup_ratio=0.1, max_length=160):\n",
        "    # Re-tokenize for this max_length\n",
        "    tr = tokenize_texts(X_train, max_length=max_length)\n",
        "    va = tokenize_texts(X_val,   max_length=max_length)\n",
        "\n",
        "    train_ds_local = Dataset.from_dict({\n",
        "        \"input_ids\": tr[\"input_ids\"],\n",
        "        \"attention_mask\": tr[\"attention_mask\"],\n",
        "        \"labels\": torch.tensor(y_train, dtype=torch.long)\n",
        "    })\n",
        "    val_ds_local = Dataset.from_dict({\n",
        "        \"input_ids\": va[\"input_ids\"],\n",
        "        \"attention_mask\": va[\"attention_mask\"],\n",
        "        \"labels\": torch.tensor(y_val, dtype=torch.long)\n",
        "    })\n",
        "\n",
        "    # Load backbone with correct num_labels\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        backbone, num_labels=num_labels\n",
        "    ).to(device)\n",
        "\n",
        "    args = make_training_args(\n",
        "        name=name, batch_size=batch_size, lr=lr, epochs=epochs,\n",
        "        weight_decay=weight_decay, warmup_ratio=warmup_ratio\n",
        "    )\n",
        "\n",
        "    trainer = WeightedTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds_local,\n",
        "        eval_dataset=val_ds_local,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    metrics = trainer.evaluate()\n",
        "    print(f\"\\n>>> {name} results: {metrics}\\n\")\n",
        "    return metrics, trainer\n",
        "\n",
        "# --- Define backbones (already set earlier) ---\n",
        "CLINICAL_BERT = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "DISTIL_BERT   = \"distilbert-base-uncased\"\n",
        "\n",
        "results = OrderedDict()\n",
        "\n",
        "# Exp-A: ClinicalBERT, conservative LR, small batch\n",
        "results['expA_clinicalbert_bs16_lr2e-5_ep3'] = run_experiment(\n",
        "    name=\"expA_clinicalbert_bs16_lr2e-5_ep3\",\n",
        "    backbone=CLINICAL_BERT,\n",
        "    batch_size=16, lr=2e-5, epochs=3,\n",
        "    weight_decay=0.01, warmup_ratio=0.1, max_length=160\n",
        ")\n",
        "\n",
        "# Exp-B: ClinicalBERT, slightly higher LR, more epochs\n",
        "results['expB_clinicalbert_bs16_lr5e-5_ep4'] = run_experiment(\n",
        "    name=\"expB_clinicalbert_bs16_lr5e-5_ep4\",\n",
        "    backbone=CLINICAL_BERT,\n",
        "    batch_size=16, lr=5e-5, epochs=4,\n",
        "    weight_decay=0.01, warmup_ratio=0.06, max_length=160\n",
        ")\n",
        "\n",
        "# Exp-C: DistilBERT fast baseline\n",
        "results['expC_distilbert_bs32_lr3e-5_ep3'] = run_experiment(\n",
        "    name=\"expC_distilbert_bs32_lr3e-5_ep3\",\n",
        "    backbone=DISTIL_BERT,\n",
        "    batch_size=32, lr=3e-5, epochs=3,\n",
        "    weight_decay=0.01, warmup_ratio=0.1, max_length=128\n",
        ")\n",
        "\n",
        "# Leaderboard\n",
        "board = []\n",
        "for k,(m,_t) in results.items():\n",
        "    board.append((k, m.get('eval_f1', float('nan')), m.get('eval_accuracy', float('nan'))))\n",
        "board = sorted(board, key=lambda x: x[1], reverse=True)\n",
        "print(\"\\nLeaderboard (by F1):\")\n",
        "for name, f1, acc in board:\n",
        "    print(f\"{name:35s}  F1={f1:.4f}  Acc={acc:.4f}\")\n"
      ],
      "id": "LkYDOan1hZ2O"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Eval (Pick Best and Run Inference)\n",
        "\n",
        "**Function Description:**\n",
        "This cell identifies the best-performing experiment from your fine-tuning runs, saves that model to disk for future use, and demonstrates how to make predictions on new text. It shows you the complete inference pipeline from raw text to predicted class and confidence scores.\n",
        "\n",
        "**Syntax Explanation:**\n",
        "The selection logic iterates through the results dictionary using `.items()` which gives you both the experiment name and its (metrics, trainer) tuple. For each experiment, I check if its `eval_f1` score beats the current best, and if so, update both `best_f1` and `best_name` while storing the trainer object. After finding the winner, `trainer.save_model()` writes the model weights to disk at the specified path, and `tokenizer.save_pretrained()` saves the tokenizer configuration alongside it. The `predict()` function encapsulates the inference pipeline - it loads the saved tokenizer with `AutoTokenizer.from_pretrained()`, loads the saved model with `AutoModelForSequenceClassification.from_pretrained()`, moves the model to the correct device with `.to(device)`, tokenizes input texts using the same parameters as training, wraps the forward pass in `torch.no_grad()` to disable gradient computation (speeds up inference and saves memory), extracts logits from model outputs, applies `torch.argmax()` to get predicted classes, applies `torch.softmax()` to convert logits to probabilities, and returns both predictions and confidence scores after moving them from GPU to CPU and converting to numpy arrays. For the demo, I define three test sentences covering different scenarios (clearly calm, clearly stressed, ambiguous) and call predict on them. The output loop uses zip to iterate over texts, predictions, and probabilities simultaneously, formatting each as a readable string with the predicted label and confidence.\n",
        "\n",
        "**Inputs:**\n",
        "This cell uses the results dictionary populated in Section 5, which contains metrics and trainer objects from all three experiments. The predict function takes a list of text strings and optionally a model directory path.\n",
        "\n",
        "**Outputs:**\n",
        "You'll see a message identifying which experiment won and what its F1 score was, followed by the save directory path. Then you'll see three prediction lines showing the predicted class (as both a number and label), the confidence probability, and the original text. For example: \"[stressed(1) p=0.873] My chest is tight and I cannot focus, I think I am very stressed.\"\n",
        "\n",
        "**Code Flow:**\n",
        "The flow divides into three phases. First, iterate through all experiment results to find the highest F1 score and corresponding trainer. Second, save both the model and tokenizer to disk. Third, demonstrate inference by defining a predict function, creating test samples, calling predict, and formatting the output. The save and load operations prove that you can persist your model and reload it later without retraining.\n",
        "\n",
        "**Comments and Observations:**\n",
        "Saving the model is important because fine-tuning takes significant time and compute resources - you don't want to retrain every time you need to make predictions. The saved directory contains multiple files including model weights (pytorch_model.bin), model configuration (config.json), and tokenizer files (vocab.txt, tokenizer_config.json). Together these files fully specify your trained model and can be loaded on any machine with the same library versions. The predict function is production-ready - you could import it into a web API or batch processing script. The `torch.no_grad()` context manager is important for inference because it tells PyTorch not to track gradients, which cuts memory usage in half and speeds up computation. The difference between logits and probabilities matters: logits are raw scores that can be any value from negative to positive infinity, while probabilities are normalized to sum to 1.0 and range from 0 to 1. Softmax converts logits to probabilities using the formula exp(logit_i) / sum(exp(logit_j)). The probability value tells you confidence - 0.95 means highly confident, 0.55 means barely confident. In production, you might set a threshold like 0.7 and only act on predictions above that threshold, sending lower-confidence predictions to human review. The three test sentences demonstrate different difficulty levels. The first (\"calm and in control\") should be easy for the model - clear language indicating low stress. The second (\"chest is tight, cannot focus, very stressed\") contains multiple stress indicators and explicitly mentions stress, so the model should confidently predict stressed. The third (\"workload is heavy but manageable\") is ambiguous - \"heavy\" suggests stress but \"manageable\" suggests coping, so this tests whether the model can handle nuance. If the model gets the easy cases right but fails on ambiguous ones, that's actually good behavior showing it's not just memorizing keywords. You can expand this demo by adding more test cases, especially edge cases like very short text (\"I'm fine\"), very long text (multiple paragraphs), or text with mixed signals. The model architecture (ClinicalBERT vs DistilBERT) affects inference speed - DistilBERT is roughly 2x faster for the same input, which matters if you're processing millions of texts."
      ],
      "metadata": {
        "id": "GuFo0oKC_Dwo"
      },
      "id": "GuFo0oKC_Dwo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSTDHJX1hZ2O"
      },
      "source": [
        "## 6) Eval (Pick Best and Run Inference)"
      ],
      "id": "MSTDHJX1hZ2O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24E_Tay_hZ2O"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Select the best run from 'results' dict above\n",
        "best_name, best_f1 = None, -1.0\n",
        "best_trainer = None\n",
        "for name,(metrics, trainer) in results.items():\n",
        "    if metrics['eval_f1'] > best_f1:\n",
        "        best_f1 = metrics['eval_f1']\n",
        "        best_name = name\n",
        "        best_trainer = trainer\n",
        "\n",
        "print(f\"Best run: {best_name} with F1={best_f1:.4f}\")\n",
        "\n",
        "# Save the best model for reuse\n",
        "save_dir = f\"./best_model_{best_name}\"\n",
        "best_trainer.save_model(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "# Simple inference helper\n",
        "def predict(texts, model_dir=save_dir):\n",
        "    tok = AutoTokenizer.from_pretrained(model_dir)\n",
        "    mdl = AutoModelForSequenceClassification.from_pretrained(model_dir).to(device)\n",
        "    enc = tok(list(texts), padding=True, truncation=True, max_length=160, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = mdl(**enc).logits\n",
        "    pred = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "    prob = torch.softmax(logits, dim=-1).cpu().numpy()[:,1]\n",
        "    return pred, prob\n",
        "\n",
        "# Demo predictions on a few samples\n",
        "samples = [\n",
        "    \"I feel calm and in control today.\",\n",
        "    \"My chest is tight and I cannot focus, I think I am very stressed.\",\n",
        "    \"Workload is heavy but manageable so far.\"\n",
        "]\n",
        "pred, prob = predict(samples)\n",
        "for s, y, p in zip(samples, pred, prob):\n",
        "    lab = \"stressed(1)\" if y==1 else \"not‚Äëstressed(0)\"\n",
        "    print(f\"[{lab}  p={p:.3f}]  {s}\")\n"
      ],
      "id": "24E_Tay_hZ2O"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "591cdd45c7b74fa181821f9529944da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_776524b1ce5c4c5bb65f8d8e65516273",
              "IPY_MODEL_3d089afa63744558bb3a6f7a1b876aef",
              "IPY_MODEL_e4a09e0a755241daad5e0330cfe68fd2"
            ],
            "layout": "IPY_MODEL_b81c22472d7940c798254fd44d9877b8"
          }
        },
        "776524b1ce5c4c5bb65f8d8e65516273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5da86772c65244f09c694822bd730ce2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1cd05c5690a743bbb2472e7b3d8a91c5",
            "value": "config.json:‚Äá100%"
          }
        },
        "3d089afa63744558bb3a6f7a1b876aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b07464960b8645bbaca506d2ef31bace",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5fde31686a35414f962235ea930bdccb",
            "value": 385
          }
        },
        "e4a09e0a755241daad5e0330cfe68fd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b95ecc756ea41f29095ab7890ab856c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5a1b98fe2c514b45a8153a30ba29225b",
            "value": "‚Äá385/385‚Äá[00:00&lt;00:00,‚Äá33.1kB/s]"
          }
        },
        "b81c22472d7940c798254fd44d9877b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5da86772c65244f09c694822bd730ce2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cd05c5690a743bbb2472e7b3d8a91c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b07464960b8645bbaca506d2ef31bace": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fde31686a35414f962235ea930bdccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b95ecc756ea41f29095ab7890ab856c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a1b98fe2c514b45a8153a30ba29225b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd7b191158e14b56b7fb18d781905220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c86017ab938b4f46baa78b381b03320c",
              "IPY_MODEL_11930ebd9ce2434c8aaf21655a0f564a",
              "IPY_MODEL_e217ae1ba3974278934fbb75fae99d57"
            ],
            "layout": "IPY_MODEL_3c9e5917c4754acbbb908f3431191d45"
          }
        },
        "c86017ab938b4f46baa78b381b03320c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c6926731b6e4c06a63e5a2ef414f6a9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_35c4939c98a141cdaecbbd978211e48d",
            "value": "vocab.txt:‚Äá"
          }
        },
        "11930ebd9ce2434c8aaf21655a0f564a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddd174a933954b4fab9f1640ad491973",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ade9e93734a1456797e0340e38182c04",
            "value": 1
          }
        },
        "e217ae1ba3974278934fbb75fae99d57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74b176f12a3d42139088e6f4dd3f2d12",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_08e6b823fa1042c9892bc49c949a2857",
            "value": "‚Äá213k/?‚Äá[00:00&lt;00:00,‚Äá7.07MB/s]"
          }
        },
        "3c9e5917c4754acbbb908f3431191d45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c6926731b6e4c06a63e5a2ef414f6a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35c4939c98a141cdaecbbd978211e48d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddd174a933954b4fab9f1640ad491973": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ade9e93734a1456797e0340e38182c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74b176f12a3d42139088e6f4dd3f2d12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08e6b823fa1042c9892bc49c949a2857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}