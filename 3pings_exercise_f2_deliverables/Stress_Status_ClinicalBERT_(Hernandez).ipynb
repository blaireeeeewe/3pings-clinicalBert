{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQE7XBmrhZ2H"
      },
      "source": [
        "\n",
        "# Stress Status Detection ‚Äî End‚Äëto‚ÄëEnd Colab Notebook  \n",
        "**Order:** Load Dataset ‚Üí Baseline Models ‚Üí Pre‚ÄëTrained Models ‚Üí Training of Data ‚Üí Fine‚Äëtuning ‚Üí Eval\n",
        "\n",
        "> This notebook encodes the provided fine‚Äëtuning script into a structured, well‚Äëcommented pipeline.  \n",
        "> It includes at least **three hyperparameter experiments** (aiming to maximize **F1‚ÄëScore**).  \n",
        "> Replace the dataset path with your CSV if needed ‚Äî required columns: `statement` (text) and `status` (0/1).\n"
      ],
      "id": "iQE7XBmrhZ2H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOwoPghthZ2K"
      },
      "source": [
        "# 0) Setup (libraries and reproducibility)"
      ],
      "id": "lOwoPghthZ2K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- Import and Environment Setup ---\n",
        "\n",
        "import os\n",
        "- used for managing file paths, environment variables, and directory operations to ensure smooth file handling throughout the notebook\n",
        "\n",
        "import math\n",
        "- provides access to mathematical functions that can be useful for adjusting learning rate schedules or calculations during training\n",
        "\n",
        "import random\n",
        "- initializes and controls random number generation to keep results consistent across multiple runs, ensuring reproducibility\n",
        "\n",
        "import numpy as np\n",
        "- supports numerical computations, array operations, and helps manage data processing tasks efficiently before feeding data to the model\n",
        "\n",
        "import pandas as pd\n",
        "- enables loading, inspecting, and manipulating structured data such as CSV files, which is essential for preparing datasets for model training\n",
        "\n",
        "from pathlib import Path\n",
        "- offers an object-oriented approach to handle file and directory paths in a more readable and cross-platform manner\n",
        "\n",
        "--- Core Framework Imports ---\n",
        "\n",
        "import torch\n",
        "- serves as the foundation for tensor operations and deep learning on both CPU and GPU, allowing efficient model training and inference\n",
        "\n",
        "from datasets import Dataset\n",
        "- converts pandas DataFrames into an optimized dataset format compatible with Hugging Face Transformers for seamless data management\n",
        "\n",
        "from transformers import (\n",
        "AutoTokenizer,\n",
        "- automatically retrieves the correct tokenizer corresponding to the specified model checkpoint, ensuring token compatibility\n",
        "AutoModelForSequenceClassification,\n",
        "- loads a pre-trained Transformer model with an added classification layer suitable for text classification tasks\n",
        "TrainingArguments,\n",
        " - defines and stores hyperparameters for the training process such as epochs, batch size, and evaluation strategy\n",
        "Trainer\n",
        "- manages the full training loop, including evaluation, logging, and checkpoint saving, simplifying fine-tuning workflows\n",
        ")\n",
        "\n",
        "--- Evaluation Metric Imports ---\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        " - provides standard metrics to measure model performance, including accuracy, precision, recall, and F1-score for balanced evaluation\n",
        "\n",
        "--- Reproducibility Configuration ---\n",
        "\n",
        "SEED = 42\n",
        "- assigns a fixed seed value to maintain consistency across random operations for all libraries used\n",
        "random.seed(SEED)\n",
        " - ensures Python‚Äôs random processes produce the same outcomes each run\n",
        "np.random.seed(SEED)\n",
        " - synchronizes NumPy‚Äôs internal random state for repeatable data shuffling and sampling\n",
        "torch.manual_seed(SEED)\n",
        "- locks PyTorch‚Äôs random state for deterministic model initialization\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        " - extends reproducibility settings to all available GPUs, ensuring consistent results even in multi-GPU environments\n",
        "\n",
        "--- Device Detection ---\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "- checks if a GPU is available; otherwise defaults to CPU for computation\n",
        "print(f\"Using device: {device}\")\n",
        "- displays which hardware is being used to verify that GPU acceleration is active when available"
      ],
      "metadata": {
        "id": "lXM6q7RwB9PV"
      },
      "id": "lXM6q7RwB9PV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1V0utC5hZ2L",
        "outputId": "e0e1464a-b307-427c-d5ab-9d1e0194bcda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Every import has an explanatory comment.\n",
        "import os                         # file paths and environment checks\n",
        "import math                       # math helpers (may be useful for schedules)\n",
        "import random                     # Python's RNG for reproducibility\n",
        "import numpy as np                # numerical arrays and metrics support\n",
        "import pandas as pd               # data loading and manipulation\n",
        "from pathlib import Path          # convenient and robust path handling\n",
        "\n",
        "# Hugging Face / PyTorch stack (for transformer fine‚Äëtuning)\n",
        "import torch                      # tensor and GPU utilities\n",
        "from datasets import Dataset      # lightweight dataset wrapper around pandas\n",
        "from transformers import (       # core HF components for tokenization and training\n",
        "    AutoTokenizer,               # auto‚Äëloads the right tokenizer for a given model checkpoint\n",
        "    AutoModelForSequenceClassification,  # classification head on top of a transformer\n",
        "    TrainingArguments,           # training hyperparameters container\n",
        "    Trainer                      # training loop helper (handles eval and logging)\n",
        ")\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Make runs reproducible (seed Python, NumPy, and PyTorch)\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Detect device once and print for visibility\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")  # shows 'cuda' when a GPU is available in Colab\n"
      ],
      "id": "z1V0utC5hZ2L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ3rjAL2hZ2L"
      },
      "source": [
        "## 1) Load Dataset"
      ],
      "id": "wQ3rjAL2hZ2L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Load Dataset (Upload version, auto-encodes text labels) ---\n",
        "\n",
        "#import pandas as pd  \n",
        "- loads the pandas library for handling CSV data\n",
        "\n",
        "#from pathlib import Path  \n",
        "- allows robust and convenient file path handling\n",
        "\n",
        "#from google.colab import files  \n",
        "- enables file upload directly in Google Colab\n",
        "\n",
        "#print(\"üìÇ Please upload your dataset CSV (e.g., Combined Data.csv)\")  \n",
        "- displays an instruction message asking the user to upload the dataset\n",
        "\n",
        "#uploaded = files.upload()  \n",
        "- opens a file upload dialog for selecting the CSV file\n",
        "\n",
        "#filename = list(uploaded.keys())[0]  \n",
        "- retrieves the name of the first uploaded file\n",
        "\n",
        "#csv_path = Path(f\"/content/{filename}\")  \n",
        "- constructs the full path to the uploaded file in Colab‚Äôs environment\n",
        "\n",
        "#print(f\"‚úÖ File uploaded successfully: {csv_path}\")  \n",
        "- confirms successful upload and shows the file path\n",
        "\n",
        "#df = pd.read_csv(csv_path)  \n",
        "- reads the uploaded CSV file into a pandas DataFrame\n",
        "\n",
        "# --- Validate columns ---\n",
        "\n",
        "#expected_cols = {'statement', 'status'}  \n",
        "- defines the expected column names required in the dataset\n",
        "\n",
        "#assert expected_cols.issubset(df.columns), f\"‚ùå Missing required columns: {expected_cols - set(df.columns)}\"  \n",
        "- checks if required columns exist, otherwise stops execution with an error\n",
        "\n",
        "# --- Clean ---\n",
        "\n",
        "#df = df.dropna(subset=['statement', 'status']).copy()  \n",
        "- removes rows that have missing values in the statement or status columns\n",
        "\n",
        "#df['statement'] = df['statement'].astype(str)  \n",
        "- ensures all text entries in the statement column are strings\n",
        "\n",
        "# --- Encode text labels into integers ---\n",
        "\n",
        "#from sklearn.preprocessing import LabelEncoder  \n",
        "- imports a utility that converts categorical text labels into numeric values\n",
        "\n",
        "#le = LabelEncoder()  \n",
        "- creates an instance of the LabelEncoder class\n",
        "\n",
        "#df['status_encoded'] = le.fit_transform(df['status'])  \n",
        "- fits the encoder on the status column and creates a new encoded column\n",
        "\n",
        "#print(\"üî§ Label encoding map:\")  \n",
        "- prints a heading for the label mapping information\n",
        "\n",
        "#for label, code in zip(le.classes_, range(len(le.classes_))):  \n",
        "- iterates through each label and its assigned numeric code\n",
        "    print(f\"  {code} ‚Üí {label}\")  # displays each code-label pair\n",
        "\n",
        "#df['status'] = df['status_encoded']  \n",
        "- replaces the original text labels with numeric values\n",
        "\n",
        "#df.drop(columns=['status_encoded'], inplace=True)  \n",
        "- removes the temporary encoded column since it‚Äôs no longer needed\n",
        "\n",
        "#print(\"\\n‚úÖ Dataset loaded and label-encoded successfully!\")  \n",
        "- prints confirmation that data cleaning and encoding are complete\n",
        "\n",
        "#print(df['status'].value_counts(dropna=False))  \n",
        "- shows the number of samples per label for verification\n",
        "\n",
        "#df.head(3)  \n",
        "- displays the first three rows of the cleaned and processed dataset\n"
      ],
      "metadata": {
        "id": "UKbddZcLB-e5"
      },
      "id": "UKbddZcLB-e5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "CMBpEom1hZ2M",
        "outputId": "0b419b6c-46d6-4b46-89e7-1b34b216b0b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Please upload your dataset CSV (e.g., Combined Data.csv)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4da332ec-4edb-4bfb-9dc0-c6593407b70f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4da332ec-4edb-4bfb-9dc0-c6593407b70f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Combined Data.csv to Combined Data (2).csv\n",
            "‚úÖ File uploaded successfully: /content/Combined Data (2).csv\n",
            "üî§ Label encoding map:\n",
            "  0 ‚Üí Anxiety\n",
            "  1 ‚Üí Bipolar\n",
            "  2 ‚Üí Depression\n",
            "  3 ‚Üí Normal\n",
            "  4 ‚Üí Personality disorder\n",
            "  5 ‚Üí Stress\n",
            "  6 ‚Üí Suicidal\n",
            "\n",
            "‚úÖ Dataset loaded and label-encoded successfully!\n",
            "status\n",
            "3    16343\n",
            "2    15404\n",
            "6    10652\n",
            "0     3841\n",
            "1     2777\n",
            "5     2587\n",
            "4     1077\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                          statement  status\n",
              "0           0                                         oh my gosh       0\n",
              "1           1  trouble sleeping, confused mind, restless hear...       0\n",
              "2           2  All wrong, back off dear, forward doubt. Stay ...       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57642578-bac5-498e-bc81-5c4d7597b015\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>statement</th>\n",
              "      <th>status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>oh my gosh</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>trouble sleeping, confused mind, restless hear...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>All wrong, back off dear, forward doubt. Stay ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57642578-bac5-498e-bc81-5c4d7597b015')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-57642578-bac5-498e-bc81-5c4d7597b015 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-57642578-bac5-498e-bc81-5c4d7597b015');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f6b88fae-1411-4565-a5ae-000d91305bfd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f6b88fae-1411-4565-a5ae-000d91305bfd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f6b88fae-1411-4565-a5ae-000d91305bfd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 52681,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15235,\n        \"min\": 0,\n        \"max\": 53042,\n        \"num_unique_values\": 52681,\n        \"samples\": [\n          3008,\n          44705,\n          50186\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"statement\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 51073,\n        \"samples\": [\n          \"he's been a chain smoker for 30 years.\",\n          \"Dependence on therapist I attend IOP groups and individual therapy sessions at the same place, my therapist who I have worked with on and off for a year and a couple months just told me today that she is leaving soon and I am heartbroken. I love my therapist and I don't know how I am going to keep progressing without her. There will be a replacement for her but idk what to do, I don't want a different therapist. :(\",\n          \"These feelings constantly come back. Someone from my past that hurt me came back a month ago and once again disrespected me and i just feel like shit. Idk why these feelings keep resurfacing but it just hurts. I do not want to be over dramatic but Its hurts when you were nothing but loving/kind to someone and they disrespect you. I just hate feeling like this, feeling like i cannot trust anyone or that no one would ever truly love me unless i have something to offer. I am always worried about my looks and its just making me depressed. I really do not feel like i fit in with the world I am just here. Idk what my next step should be to get help but I am really going through it. (Yes I am in therapy) but how do i help myself ? I have been depressed/anxious for years and most day i do not even leave my house. But nobody around me seems to care and honestly I am tired of feeling this way. But at the same time i do not want to give up on myself bc i feel like I am here to be somebody great. I am just trying to find my way right now. It keeps coming back\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# --- Load Dataset (Upload version, auto-encodes text labels) ---\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "print(\"üìÇ Please upload your dataset CSV (e.g., Combined Data.csv)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Automatically pick the first uploaded file\n",
        "filename = list(uploaded.keys())[0]\n",
        "csv_path = Path(f\"/content/{filename}\")\n",
        "\n",
        "print(f\"‚úÖ File uploaded successfully: {csv_path}\")\n",
        "\n",
        "# Load the CSV\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# --- Validate columns ---\n",
        "expected_cols = {'statement', 'status'}\n",
        "assert expected_cols.issubset(df.columns), f\"‚ùå Missing required columns: {expected_cols - set(df.columns)}\"\n",
        "\n",
        "# --- Clean ---\n",
        "df = df.dropna(subset=['statement', 'status']).copy()\n",
        "df['statement'] = df['statement'].astype(str)\n",
        "\n",
        "# --- Encode text labels into integers ---\n",
        "# This maps each unique label (like 'Anxiety', 'Stress', etc.) to a numeric ID\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['status_encoded'] = le.fit_transform(df['status'])\n",
        "\n",
        "# Optional: print mapping for your reference\n",
        "print(\"üî§ Label encoding map:\")\n",
        "for label, code in zip(le.classes_, range(len(le.classes_))):\n",
        "    print(f\"  {code} ‚Üí {label}\")\n",
        "\n",
        "# Replace 'status' with the encoded version\n",
        "df['status'] = df['status_encoded']\n",
        "df.drop(columns=['status_encoded'], inplace=True)\n",
        "\n",
        "print(\"\\n‚úÖ Dataset loaded and label-encoded successfully!\")\n",
        "print(df['status'].value_counts(dropna=False))\n",
        "df.head(3)\n"
      ],
      "id": "CMBpEom1hZ2M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymAj-xUdhZ2M"
      },
      "source": [
        "## 2) Baseline Models (TF‚ÄëIDF + Linear)"
      ],
      "id": "ymAj-xUdhZ2M"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Baseline Models (TF-IDF + Linear, supports multi-class) ---\n",
        "\n",
        "#from sklearn.model_selection import train_test_split  \n",
        "- splits the dataset into training and validation sets\n",
        "\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer  \n",
        "- converts text into numerical TF-IDF feature vectors\n",
        "\n",
        "#from sklearn.linear_model import LogisticRegression  \n",
        "- imports the logistic regression model for classification\n",
        "\n",
        "#from sklearn.svm import LinearSVC  \n",
        "- imports the linear support vector machine classifier\n",
        "\n",
        "#from sklearn.metrics import accuracy_score, precision_recall_fscore_support  \n",
        "- provides functions for evaluating model performance\n",
        "\n",
        "#import numpy as np  \n",
        "- used for numerical operations and array manipulation\n",
        "\n",
        "#X_train, X_val, y_train, y_val = train_test_split(  \n",
        "#    df['statement'].values,  \n",
        "#    df['status'].values,  \n",
        "#    test_size=0.2,  \n",
        "#    random_state=42,  \n",
        "#    stratify=df['status'].values  \n",
        "#)  \n",
        "- splits data into 80% training and 20% validation sets while keeping label proportions balanced\n",
        "\n",
        "#tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=2, max_features=40000)  \n",
        "- creates a TF-IDF vectorizer using unigrams and bigrams with up to 40,000 features\n",
        "\n",
        "#Xtr = tfidf.fit_transform(X_train)  \n",
        "- fits the TF-IDF model on training text and transforms it into feature vectors\n",
        "\n",
        "#Xva = tfidf.transform(X_val)  \n",
        "- applies the same transformation to the validation set without refitting\n",
        "\n",
        "#num_classes = len(np.unique(y_train))  \n",
        "- counts the number of unique labels in the training data\n",
        "\n",
        "#avg_type = \"binary\" if num_classes == 2 else \"weighted\"  \n",
        "- selects metric averaging type depending on whether it‚Äôs binary or multi-class\n",
        "\n",
        "#print(f\"Detected {num_classes} classes ‚Üí using average='{avg_type}' for metrics.\\n\")  \n",
        "- prints the detected number of classes and chosen averaging method\n",
        "\n",
        "# --- Baseline 1: Logistic Regression ---\n",
        "\n",
        "#logreg = LogisticRegression(max_iter=2000, class_weight=\"balanced\")  \n",
        "- initializes logistic regression with balanced class weights and higher iteration limit\n",
        "\n",
        "#logreg.fit(Xtr, y_train)  \n",
        "- trains the logistic regression model on TF-IDF features\n",
        "\n",
        "#pred_lr = logreg.predict(Xva)  \n",
        "- generates predictions on the validation data\n",
        "\n",
        "#p, r, f, _ = precision_recall_fscore_support(y_val, pred_lr, average=avg_type)  \n",
        "- computes precision, recall, and F1-score using the chosen averaging type\n",
        "\n",
        "#acc = accuracy_score(y_val, pred_lr)  \n",
        "- calculates overall accuracy of the logistic regression model\n",
        "\n",
        "#print(f\"[Baseline-LR] Acc={acc:.3f}  P={p:.3f}  R={r:.3f}  F1={f:.3f}\")  \n",
        "- displays evaluation metrics for the logistic regression baseline\n",
        "\n",
        "# --- Baseline 2: Linear SVM ---\n",
        "\n",
        "#svm = LinearSVC(class_weight=\"balanced\")  \n",
        "- initializes a linear SVM classifier with balanced class weights\n",
        "\n",
        "#svm.fit(Xtr, y_train)  \n",
        "- trains the SVM model on the training data\n",
        "\n",
        "#pred_svm = svm.predict(Xva)  \n",
        "- predicts labels for the validation set\n",
        "\n",
        "#p, r, f, _ = precision_recall_fscore_support(y_val, pred_svm, average=avg_type)  \n",
        "- calculates precision, recall, and F1-score for SVM predictions\n",
        "\n",
        "#acc = accuracy_score(y_val, pred_svm)  \n",
        "- computes accuracy of the SVM model\n",
        "\n",
        "#print(f\"[Baseline-SVM] Acc={acc:.3f}  P={p:.3f}  R={r:.3f}  F1={f:.3f}\")  \n",
        "- displays evaluation metrics for the SVM baseline model\n"
      ],
      "metadata": {
        "id": "xNWyoNu3CBhm"
      },
      "id": "xNWyoNu3CBhm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4IjGl_JhZ2M",
        "outputId": "9664c070-a5a5-4a8b-eed7-3d14618b933f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected 7 classes ‚Üí using average='weighted' for metrics.\n",
            "\n",
            "[Baseline-LR] Acc=0.778  P=0.787  R=0.778  F1=0.777\n",
            "[Baseline-SVM] Acc=0.782  P=0.779  R=0.782  F1=0.780\n"
          ]
        }
      ],
      "source": [
        "# --- Baseline Models (TF-IDF + Linear, supports multi-class) ---\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "# Train/validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df['statement'].values,\n",
        "    df['status'].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['status'].values\n",
        ")\n",
        "\n",
        "# Convert raw text into TF-IDF features\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=2, max_features=40000)\n",
        "Xtr = tfidf.fit_transform(X_train)\n",
        "Xva = tfidf.transform(X_val)\n",
        "\n",
        "# Detect if this is binary or multiclass\n",
        "num_classes = len(np.unique(y_train))\n",
        "avg_type = \"binary\" if num_classes == 2 else \"weighted\"\n",
        "print(f\"Detected {num_classes} classes ‚Üí using average='{avg_type}' for metrics.\\n\")\n",
        "\n",
        "# --- Baseline 1: Logistic Regression ---\n",
        "logreg = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
        "logreg.fit(Xtr, y_train)\n",
        "pred_lr = logreg.predict(Xva)\n",
        "p, r, f, _ = precision_recall_fscore_support(y_val, pred_lr, average=avg_type)\n",
        "acc = accuracy_score(y_val, pred_lr)\n",
        "print(f\"[Baseline-LR] Acc={acc:.3f}  P={p:.3f}  R={r:.3f}  F1={f:.3f}\")\n",
        "\n",
        "# --- Baseline 2: Linear SVM ---\n",
        "svm = LinearSVC(class_weight=\"balanced\")\n",
        "svm.fit(Xtr, y_train)\n",
        "pred_svm = svm.predict(Xva)\n",
        "p, r, f, _ = precision_recall_fscore_support(y_val, pred_svm, average=avg_type)\n",
        "acc = accuracy_score(y_val, pred_svm)\n",
        "print(f\"[Baseline-SVM] Acc={acc:.3f}  P={p:.3f}  R={r:.3f}  F1={f:.3f}\")\n"
      ],
      "id": "k4IjGl_JhZ2M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xic-iii4hZ2N"
      },
      "source": [
        "## 3) Pre‚ÄëTrained Models (Tokenization and Dataset Prep)"
      ],
      "id": "xic-iii4hZ2N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Transformer Backbone and Tokenization Setup ---\n",
        "\n",
        "#CLINICAL_BERT = \"emilyalsentzer/Bio_ClinicalBERT\"  \n",
        "- defines the pretrained ClinicalBERT model specialized for clinical or medical text\n",
        "\n",
        "#DISTIL_BERT   = \"distilbert-base-uncased\"  \n",
        "- defines the lightweight DistilBERT model for faster fine-tuning and baseline comparison\n",
        "\n",
        "#BACKBONE = CLINICAL_BERT  \n",
        "- sets ClinicalBERT as the default model backbone for experiments\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(BACKBONE)  \n",
        "- loads the tokenizer that matches the chosen transformer backbone\n",
        "\n",
        "#def tokenize_texts(texts, max_length=128):  \n",
        "- defines a function to tokenize a list or series of input texts\n",
        "    #return tokenizer(  \n",
        "    #    list(texts),                 # a Python list of strings  \n",
        "    #    padding=True,                # pads shorter texts to the same length  \n",
        "    #    truncation=True,             # trims texts that exceed max_length  \n",
        "    #    max_length=max_length,       # sets a limit on sequence length  \n",
        "    #    return_tensors=\"pt\"          # returns tensors compatible with PyTorch  \n",
        "    #)  \n",
        "- applies the tokenizer to input texts and returns encoded tensors ready for model input\n",
        "\n",
        "#train_enc = tokenize_texts(X_train)  \n",
        "- tokenizes the training dataset to produce input IDs and attention masks\n",
        "\n",
        "#val_enc   = tokenize_texts(X_val)  \n",
        "- tokenizes the validation dataset using the same tokenizer settings\n",
        "\n",
        "#train_ds = Dataset.from_dict({  \n",
        "#    \"input_ids\": train_enc[\"input_ids\"],  \n",
        "#    \"attention_mask\": train_enc[\"attention_mask\"],  \n",
        "#    \"labels\": torch.tensor(y_train)  \n",
        "#})  \n",
        "- creates a Hugging Face Dataset object for the training data containing input tensors and labels\n",
        "\n",
        "#val_ds = Dataset.from_dict({  \n",
        "#    \"input_ids\": val_enc[\"input_ids\"],  \n",
        "#    \"attention_mask\": val_enc[\"attention_mask\"],  \n",
        "#    \"labels\": torch.tensor(y_val)  \n",
        "#})  \n",
        "- creates a similar Dataset object for the validation data\n",
        "\n",
        "#len(train_ds), len(val_ds)  \n",
        "- checks the number of samples in both training and validation datasets\n"
      ],
      "metadata": {
        "id": "SBeklvMECK6W"
      },
      "id": "SBeklvMECK6W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222,
          "referenced_widgets": [
            "591cdd45c7b74fa181821f9529944da2",
            "776524b1ce5c4c5bb65f8d8e65516273",
            "3d089afa63744558bb3a6f7a1b876aef",
            "e4a09e0a755241daad5e0330cfe68fd2",
            "b81c22472d7940c798254fd44d9877b8",
            "5da86772c65244f09c694822bd730ce2",
            "1cd05c5690a743bbb2472e7b3d8a91c5",
            "b07464960b8645bbaca506d2ef31bace",
            "5fde31686a35414f962235ea930bdccb",
            "4b95ecc756ea41f29095ab7890ab856c",
            "5a1b98fe2c514b45a8153a30ba29225b",
            "bd7b191158e14b56b7fb18d781905220",
            "c86017ab938b4f46baa78b381b03320c",
            "11930ebd9ce2434c8aaf21655a0f564a",
            "e217ae1ba3974278934fbb75fae99d57",
            "3c9e5917c4754acbbb908f3431191d45",
            "1c6926731b6e4c06a63e5a2ef414f6a9",
            "35c4939c98a141cdaecbbd978211e48d",
            "ddd174a933954b4fab9f1640ad491973",
            "ade9e93734a1456797e0340e38182c04",
            "74b176f12a3d42139088e6f4dd3f2d12",
            "08e6b823fa1042c9892bc49c949a2857"
          ]
        },
        "id": "CJsRDxIlhZ2N",
        "outputId": "baf876dd-c9d2-4b5e-8220-70328e404cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "591cdd45c7b74fa181821f9529944da2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd7b191158e14b56b7fb18d781905220"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42144, 10537)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "\n",
        "# Choose your checkpoints.\n",
        "# We include ClinicalBERT (for clinical text) and DistilBERT (fast baseline).\n",
        "CLINICAL_BERT = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "DISTIL_BERT   = \"distilbert-base-uncased\"\n",
        "\n",
        "# Pick one as the default backbone for experiments below.\n",
        "BACKBONE = CLINICAL_BERT\n",
        "\n",
        "# Initialize tokenizer for the chosen backbone\n",
        "tokenizer = AutoTokenizer.from_pretrained(BACKBONE)\n",
        "\n",
        "# Helper to tokenize a pandas series with per-line comments\n",
        "def tokenize_texts(texts, max_length=128):\n",
        "    # Apply the tokenizer: returns dict with input_ids and attention_mask\n",
        "    return tokenizer(\n",
        "        list(texts),                 # a Python list of strings\n",
        "        padding=True,                # pad to the longest in the batch\n",
        "        truncation=True,             # cut off text exceeding max_length\n",
        "        max_length=max_length,       # cap sequence length\n",
        "        return_tensors=\"pt\"          # return PyTorch tensors\n",
        "    )\n",
        "\n",
        "# Tokenize train/validation splits\n",
        "train_enc = tokenize_texts(X_train)\n",
        "val_enc   = tokenize_texts(X_val)\n",
        "\n",
        "# Wrap into HF Datasets with labels\n",
        "train_ds = Dataset.from_dict({\n",
        "    \"input_ids\": train_enc[\"input_ids\"],\n",
        "    \"attention_mask\": train_enc[\"attention_mask\"],\n",
        "    \"labels\": torch.tensor(y_train)\n",
        "})\n",
        "val_ds = Dataset.from_dict({\n",
        "    \"input_ids\": val_enc[\"input_ids\"],\n",
        "    \"attention_mask\": val_enc[\"attention_mask\"],\n",
        "    \"labels\": torch.tensor(y_val)\n",
        "})\n",
        "\n",
        "len(train_ds), len(val_ds)\n"
      ],
      "id": "CJsRDxIlhZ2N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A6SNywxhZ2N"
      },
      "source": [
        "## 4) Training of Data (Trainer utilities and metrics)"
      ],
      "id": "2A6SNywxhZ2N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Metric Computation and Weighted Trainer ---\n",
        "\n",
        "#def compute_metrics(eval_pred):  \n",
        "- defines a function to calculate evaluation metrics during training\n",
        "\n",
        "#logits, labels = eval_pred  \n",
        "- unpacks model predictions (logits) and true labels from the evaluation results\n",
        "\n",
        "#preds = np.argmax(logits, axis=-1)  \n",
        "- selects the class with the highest prediction score for each sample\n",
        "\n",
        "#precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")  \n",
        "- computes precision, recall, and F1-score assuming binary classification\n",
        "\n",
        "#acc = accuracy_score(labels, preds)  \n",
        "- calculates overall accuracy between predictions and true labels\n",
        "\n",
        "#return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}  \n",
        "- returns a dictionary of all computed metric values to the Trainer\n",
        "\n",
        "#pos = (y_train == 1).sum()  \n",
        "- counts how many positive samples are in the training data\n",
        "\n",
        "#neg = (y_train == 0).sum()  \n",
        "- counts how many negative samples are in the training data\n",
        "\n",
        "#w_pos = neg / max(pos, 1)  \n",
        "- sets the positive class weight inversely proportional to its frequency\n",
        "\n",
        "#w_neg = 1.0  \n",
        "- assigns a baseline weight of 1 to the negative class\n",
        "\n",
        "#class_weights = torch.tensor([w_neg, w_pos], dtype=torch.float).to(device)  \n",
        "- creates a tensor of class weights and moves it to the selected device (CPU or GPU)\n",
        "\n",
        "#print(f\"Class weights (neg, pos): {class_weights.tolist()}\")  \n",
        "- displays the computed class weights for reference\n",
        "\n",
        "#from torch.nn import CrossEntropyLoss  \n",
        "- imports the cross-entropy loss function used for classification tasks\n",
        "\n",
        "#class WeightedTrainer(Trainer):  \n",
        "- defines a custom Trainer subclass that supports class-weighted loss\n",
        "\n",
        "#def compute_loss(self, model, inputs, return_outputs=False):  \n",
        "- overrides the Trainer‚Äôs default loss computation method\n",
        "\n",
        "#labels = inputs.get(\"labels\")  \n",
        "- extracts the ground-truth labels from the input batch\n",
        "\n",
        "#outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})  \n",
        "- runs a forward pass of the model using the input tensors (excluding labels)\n",
        "\n",
        "#logits = outputs.get(\"logits\")  \n",
        "- retrieves raw model output scores before activation\n",
        "\n",
        "#loss_fct = CrossEntropyLoss(weight=class_weights)  \n",
        "- initializes cross-entropy loss with the specified class weights\n",
        "\n",
        "#loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))  \n",
        "- computes the weighted loss comparing predictions to true labels\n",
        "\n",
        "#return (loss, outputs) if return_outputs else loss  \n",
        "- returns both loss and model outputs if requested, otherwise just the loss value\n"
      ],
      "metadata": {
        "id": "rUDnzZ1_CMLD"
      },
      "id": "rUDnzZ1_CMLD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_4oBKl6hZ2N",
        "outputId": "66d05985-aab7-4927-d23a-7341b0cd56be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights (neg, pos): [1.0, 1.3836109638214111]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Metric function for the Trainer: computes Accuracy, Precision, Recall, F1\n",
        "def compute_metrics(eval_pred):\n",
        "    # eval_pred is a tuple of (logits, labels)\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# Optional: class weights for imbalanced datasets\n",
        "# Compute weights inversely proportional to class frequencies\n",
        "pos = (y_train == 1).sum()\n",
        "neg = (y_train == 0).sum()\n",
        "w_pos = neg / max(pos, 1)   # weight for positive class\n",
        "w_neg = 1.0                 # keep negative as baseline\n",
        "class_weights = torch.tensor([w_neg, w_pos], dtype=torch.float).to(device)\n",
        "print(f\"Class weights (neg, pos): {class_weights.tolist()}\" )\n",
        "\n",
        "# Custom Trainer that injects weighted loss\n",
        "from torch.nn import CrossEntropyLoss\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = CrossEntropyLoss(weight=class_weights)\n",
        "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ],
      "id": "9_4oBKl6hZ2N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvrjk1PlhZ2N"
      },
      "source": [
        "## 5) Fine‚Äëtuning (Three Experiments)"
      ],
      "id": "Dvrjk1PlhZ2N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- 5) Fine-tuning (Three Experiments) [version-compatible] ---\n",
        "\n",
        "#import numpy as np  \n",
        "- provides support for array operations and numerical computations\n",
        "\n",
        "#import torch  \n",
        "- enables tensor computation and GPU acceleration\n",
        "\n",
        "#from collections import OrderedDict  \n",
        "- maintains insertion order for storing results in a dictionary\n",
        "\n",
        "#from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer  \n",
        "- imports Hugging Face components for model loading, configuration, and training\n",
        "\n",
        "#from torch.nn import CrossEntropyLoss  \n",
        "- imports the loss function used for classification tasks\n",
        "\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
        "- sets computation to GPU if available, otherwise CPU\n",
        "\n",
        "#num_labels = len(np.unique(y_train))  \n",
        "- counts how many unique classes exist in the training set\n",
        "\n",
        "#avg_type = \"binary\" if num_labels == 2 else \"weighted\"  \n",
        "- decides the metric averaging type based on whether the task is binary or multiclass\n",
        "\n",
        "#print(f\"[Fine-tune] Detected {num_labels} classes ‚Üí metrics average='{avg_type}'\")  \n",
        "- prints the number of detected classes and chosen averaging method\n",
        "\n",
        "#def compute_metrics(eval_pred):  \n",
        "- defines a function to compute evaluation metrics for each epoch\n",
        "    #logits, labels = eval_pred  \n",
        "    - extracts predicted logits and true labels\n",
        "    #preds = np.argmax(logits, axis=-1)  \n",
        "    - converts logits into class predictions by selecting the highest score\n",
        "    #from sklearn.metrics import accuracy_score, precision_recall_fscore_support  \n",
        "    - imports metrics used for evaluation\n",
        "    #p, r, f, _ = precision_recall_fscore_support(labels, preds, average=avg_type)  \n",
        "    - calculates precision, recall, and F1 using the chosen averaging strategy\n",
        "    #acc = accuracy_score(labels, preds)  \n",
        "    - computes overall prediction accuracy\n",
        "    #return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f}  \n",
        "    - returns a dictionary of computed metric values\n",
        "\n",
        "#counts = np.bincount(y_train, minlength=num_labels)  \n",
        "- counts occurrences of each class label in the training data\n",
        "\n",
        "#weights = counts.max() / np.maximum(counts, 1)  \n",
        "- computes inverse-frequency weights to balance rare classes\n",
        "\n",
        "#class_weights = torch.tensor(weights, dtype=torch.float32, device=device)  \n",
        "- converts weights into a tensor and moves them to the computation device\n",
        "\n",
        "#print(f\"[Fine-tune] Class weights: {class_weights.tolist()}\")  \n",
        "- displays the computed class weights for reference\n",
        "\n",
        "#class WeightedTrainer(Trainer):  \n",
        "- creates a custom Trainer class to support weighted loss computation\n",
        "\n",
        "#def compute_loss(self, model, inputs, return_outputs=False):  \n",
        "- overrides the default loss calculation method\n",
        "    #labels = inputs.get(\"labels\")  \n",
        "    - extracts the label tensor from the input batch\n",
        "    #outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})  \n",
        "    - performs a forward pass on the model excluding labels\n",
        "    #logits = outputs.get(\"logits\")  \n",
        "    - retrieves raw model predictions\n",
        "    #loss_fct = CrossEntropyLoss(weight=class_weights)  \n",
        "    - initializes the cross-entropy loss function with class weighting\n",
        "    #loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))  \n",
        "    - computes weighted loss comparing predictions to ground truth\n",
        "    #return (loss, outputs) if return_outputs else loss  \n",
        "    - returns both loss and outputs if required\n",
        "\n",
        "#def tokenize_texts(texts, max_length=160):  \n",
        "- defines a helper function for tokenizing text data before training\n",
        "    #return tokenizer(  \n",
        "    #    list(texts),  \n",
        "    #    padding=True,  \n",
        "    #    truncation=True,  \n",
        "    #    max_length=max_length,  \n",
        "    #    return_tensors=\"pt\"  \n",
        "    #)  \n",
        "    - applies the tokenizer with truncation, padding, and fixed max length\n",
        "\n",
        "#import inspect  \n",
        "- imports a module used to check function argument compatibility across versions\n",
        "\n",
        "#def make_training_args(name, batch_size, lr, epochs, weight_decay, warmup_ratio):  \n",
        "- defines a factory function to create version-safe training arguments\n",
        "    #kwargs_modern = dict(... )  \n",
        "    - defines parameters for newer versions of Hugging Face Transformers\n",
        "    #try:  \n",
        "    - attempts to use modern TrainingArguments configuration\n",
        "    #return TrainingArguments(**kwargs_modern)  \n",
        "    - returns the configured training arguments\n",
        "    #except TypeError:  \n",
        "    - catches errors if running an older version of Transformers\n",
        "    #print(\"[Fine-tune] Using legacy TrainingArguments fallback.\")  \n",
        "    - notifies the user that the fallback configuration is being used\n",
        "    #kwargs_legacy = dict(... )  \n",
        "    - defines parameters compatible with legacy versions\n",
        "    #return TrainingArguments(**kwargs_legacy)  \n",
        "    - returns legacy-compatible training arguments\n",
        "\n",
        "#def run_experiment(name, backbone, batch_size=16, lr=2e-5, epochs=3,  \n",
        "#                   weight_decay=0.01, warmup_ratio=0.1, max_length=160):  \n",
        "- defines a function to execute one full fine-tuning experiment\n",
        "    #tr = tokenize_texts(X_train, max_length=max_length)  \n",
        "    - tokenizes training text with the chosen sequence length\n",
        "    #va = tokenize_texts(X_val, max_length=max_length)  \n",
        "    - tokenizes validation text similarly\n",
        "    #train_ds_local = Dataset.from_dict({...})  \n",
        "    - creates a Hugging Face Dataset object for the training set\n",
        "    #val_ds_local = Dataset.from_dict({...})  \n",
        "    - creates a Dataset object for validation\n",
        "    #model = AutoModelForSequenceClassification.from_pretrained(  \n",
        "    #    backbone, num_labels=num_labels  \n",
        "    #).to(device)  \n",
        "    - loads a pretrained model with a classification head matching the number of classes\n",
        "    #args = make_training_args(... )  \n",
        "    - generates compatible training arguments using provided hyperparameters\n",
        "    #trainer = WeightedTrainer(... )  \n",
        "    - initializes the Trainer with model, datasets, tokenizer, metrics, and weighted loss\n",
        "    #trainer.train()  \n",
        "    - starts the fine-tuning process\n",
        "    #metrics = trainer.evaluate()  \n",
        "    - evaluates the model on the validation set after training\n",
        "    #print(f\"\\n>>> {name} results: {metrics}\\n\")  \n",
        "    - prints the metrics for this experiment\n",
        "    #return metrics, trainer  \n",
        "    - returns both results and the trained model instance\n",
        "\n",
        "#CLINICAL_BERT = \"emilyalsentzer/Bio_ClinicalBERT\"  \n",
        "- sets the ClinicalBERT model checkpoint for fine-tuning\n",
        "\n",
        "#DISTIL_BERT   = \"distilbert-base-uncased\"  \n",
        "- sets the DistilBERT model checkpoint as a faster baseline\n",
        "\n",
        "#results = OrderedDict()  \n",
        "- initializes an ordered dictionary to store experiment outcomes\n",
        "\n",
        "#results['expA_clinicalbert_bs16_lr2e-5_ep3'] = run_experiment(... )  \n",
        "- runs Experiment A using ClinicalBERT with small batch and low learning rate\n",
        "\n",
        "#results['expB_clinicalbert_bs16_lr5e-5_ep4'] = run_experiment(... )  \n",
        "- runs Experiment B using ClinicalBERT with higher learning rate and more epochs\n",
        "\n",
        "#results['expC_distilbert_bs32_lr3e-5_ep3'] = run_experiment(... )  \n",
        "- runs Experiment C using DistilBERT with larger batch size as a fast baseline\n",
        "\n",
        "#board = []  \n",
        "- initializes a list to hold leaderboard data\n",
        "\n",
        "#for k,(m,_t) in results.items():  \n",
        "- iterates through experiment results to extract evaluation metrics\n",
        "    #board.append((k, m.get('eval_f1', float('nan')), m.get('eval_accuracy', float('nan'))))  \n",
        "    - collects experiment name, F1-score, and accuracy\n",
        "\n",
        "#board = sorted(board, key=lambda x: x[1], reverse=True)  \n",
        "- sorts experiments in descending order by F1-score\n",
        "\n",
        "#print(\"\\nLeaderboard (by F1):\")  \n",
        "- prints the leaderboard heading\n",
        "\n",
        "#for name, f1, acc in board:  \n",
        "- loops through the leaderboard entries\n",
        "    #print(f\"{name:35s}  F1={f1:.4f}  Acc={acc:.4f}\")  \n",
        "    - prints formatted performance results for each experiment\n"
      ],
      "metadata": {
        "id": "bYaD7RDrCPdp"
      },
      "id": "bYaD7RDrCPdp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "LkYDOan1hZ2O",
        "outputId": "55215eba-a0e6-4915-947f-809800dc0e16"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4064124003.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 1) Metrics: binary vs multiclass handled automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mavg_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"binary\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"weighted\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Fine-tune] Detected {num_labels} classes ‚Üí metrics average='{avg_type}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
          ]
        }
      ],
      "source": [
        "# --- 5) Fine-tuning (Three Experiments) [version-compatible] ---\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1) Metrics: binary vs multiclass handled automatically\n",
        "num_labels = len(np.unique(y_train))\n",
        "avg_type = \"binary\" if num_labels == 2 else \"weighted\"\n",
        "print(f\"[Fine-tune] Detected {num_labels} classes ‚Üí metrics average='{avg_type}'\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "    p, r, f, _ = precision_recall_fscore_support(labels, preds, average=avg_type)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f}\n",
        "\n",
        "# 2) Class weights for imbalanced data (size == num_labels)\n",
        "counts = np.bincount(y_train, minlength=num_labels)\n",
        "# Heuristic: inverse-frequency scaled to max=1.0 (safe for CE)\n",
        "weights = counts.max() / np.maximum(counts, 1)\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32, device=device)\n",
        "print(f\"[Fine-tune] Class weights: {class_weights.tolist()}\")\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = CrossEntropyLoss(weight=class_weights)\n",
        "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# 3) Helper: tokenizer already defined above. Re-tokenize per max_length\n",
        "def tokenize_texts(texts, max_length=160):\n",
        "    return tokenizer(\n",
        "        list(texts),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# 4) Version-compatible TrainingArguments factory\n",
        "import inspect\n",
        "\n",
        "def make_training_args(name, batch_size, lr, epochs, weight_decay, warmup_ratio):\n",
        "    kwargs_modern = dict(\n",
        "        output_dir=f\"./runs/{name}\",\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        learning_rate=lr,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=weight_decay,\n",
        "        warmup_ratio=warmup_ratio,\n",
        "        logging_steps=50,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        report_to=[]\n",
        "    )\n",
        "    try:\n",
        "        # Try modern signature first\n",
        "        return TrainingArguments(**kwargs_modern)\n",
        "    except TypeError:\n",
        "        # Fallback for older transformers (no evaluation_strategy/save_strategy)\n",
        "        print(\"[Fine-tune] Using legacy TrainingArguments fallback.\")\n",
        "        kwargs_legacy = dict(\n",
        "            output_dir=f\"./runs/{name}\",\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            learning_rate=lr,\n",
        "            num_train_epochs=epochs,\n",
        "            weight_decay=weight_decay,\n",
        "            logging_steps=50,\n",
        "            do_eval=True,          # legacy way to enable evaluation\n",
        "            save_steps=500,        # periodic saving\n",
        "            overwrite_output_dir=True,\n",
        "            fp16=torch.cuda.is_available()\n",
        "        )\n",
        "        return TrainingArguments(**kwargs_legacy)\n",
        "\n",
        "def run_experiment(name, backbone, batch_size=16, lr=2e-5, epochs=3,\n",
        "                   weight_decay=0.01, warmup_ratio=0.1, max_length=160):\n",
        "    # Re-tokenize for this max_length\n",
        "    tr = tokenize_texts(X_train, max_length=max_length)\n",
        "    va = tokenize_texts(X_val,   max_length=max_length)\n",
        "\n",
        "    train_ds_local = Dataset.from_dict({\n",
        "        \"input_ids\": tr[\"input_ids\"],\n",
        "        \"attention_mask\": tr[\"attention_mask\"],\n",
        "        \"labels\": torch.tensor(y_train, dtype=torch.long)\n",
        "    })\n",
        "    val_ds_local = Dataset.from_dict({\n",
        "        \"input_ids\": va[\"input_ids\"],\n",
        "        \"attention_mask\": va[\"attention_mask\"],\n",
        "        \"labels\": torch.tensor(y_val, dtype=torch.long)\n",
        "    })\n",
        "\n",
        "    # Load backbone with correct num_labels\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        backbone, num_labels=num_labels\n",
        "    ).to(device)\n",
        "\n",
        "    args = make_training_args(\n",
        "        name=name, batch_size=batch_size, lr=lr, epochs=epochs,\n",
        "        weight_decay=weight_decay, warmup_ratio=warmup_ratio\n",
        "    )\n",
        "\n",
        "    trainer = WeightedTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds_local,\n",
        "        eval_dataset=val_ds_local,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    metrics = trainer.evaluate()\n",
        "    print(f\"\\n>>> {name} results: {metrics}\\n\")\n",
        "    return metrics, trainer\n",
        "\n",
        "# --- Define backbones (already set earlier) ---\n",
        "CLINICAL_BERT = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "DISTIL_BERT   = \"distilbert-base-uncased\"\n",
        "\n",
        "results = OrderedDict()\n",
        "\n",
        "# Exp-A: ClinicalBERT, conservative LR, small batch\n",
        "results['expA_clinicalbert_bs16_lr2e-5_ep3'] = run_experiment(\n",
        "    name=\"expA_clinicalbert_bs16_lr2e-5_ep3\",\n",
        "    backbone=CLINICAL_BERT,\n",
        "    batch_size=16, lr=2e-5, epochs=3,\n",
        "    weight_decay=0.01, warmup_ratio=0.1, max_length=160\n",
        ")\n",
        "\n",
        "# Exp-B: ClinicalBERT, slightly higher LR, more epochs\n",
        "results['expB_clinicalbert_bs16_lr5e-5_ep4'] = run_experiment(\n",
        "    name=\"expB_clinicalbert_bs16_lr5e-5_ep4\",\n",
        "    backbone=CLINICAL_BERT,\n",
        "    batch_size=16, lr=5e-5, epochs=4,\n",
        "    weight_decay=0.01, warmup_ratio=0.06, max_length=160\n",
        ")\n",
        "\n",
        "# Exp-C: DistilBERT fast baseline\n",
        "results['expC_distilbert_bs32_lr3e-5_ep3'] = run_experiment(\n",
        "    name=\"expC_distilbert_bs32_lr3e-5_ep3\",\n",
        "    backbone=DISTIL_BERT,\n",
        "    batch_size=32, lr=3e-5, epochs=3,\n",
        "    weight_decay=0.01, warmup_ratio=0.1, max_length=128\n",
        ")\n",
        "\n",
        "# Leaderboard\n",
        "board = []\n",
        "for k,(m,_t) in results.items():\n",
        "    board.append((k, m.get('eval_f1', float('nan')), m.get('eval_accuracy', float('nan'))))\n",
        "board = sorted(board, key=lambda x: x[1], reverse=True)\n",
        "print(\"\\nLeaderboard (by F1):\")\n",
        "for name, f1, acc in board:\n",
        "    print(f\"{name:35s}  F1={f1:.4f}  Acc={acc:.4f}\")\n"
      ],
      "id": "LkYDOan1hZ2O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSTDHJX1hZ2O"
      },
      "source": [
        "## 6) Eval (Pick Best and Run Inference)"
      ],
      "id": "MSTDHJX1hZ2O"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Best Model Selection, Saving, and Inference ---\n",
        "\n",
        "#best_name, best_f1 = None, -1.0  \n",
        "- initializes variables to store the best model‚Äôs name and its highest F1 score\n",
        "\n",
        "#best_trainer = None  \n",
        "- sets a placeholder for the trainer object of the best-performing run\n",
        "\n",
        "#for name,(metrics, trainer) in results.items():  \n",
        "- iterates through all fine-tuning experiment results\n",
        "    #if metrics['eval_f1'] > best_f1:  \n",
        "    - checks if the current run‚Äôs F1 score is higher than the previous best\n",
        "    #best_f1 = metrics['eval_f1']  \n",
        "    - updates the highest F1 score\n",
        "    #best_name = name  \n",
        "    - records the name of the best-performing experiment\n",
        "    #best_trainer = trainer  \n",
        "    - stores the trainer associated with that best run\n",
        "\n",
        "#print(f\"Best run: {best_name} with F1={best_f1:.4f}\")  \n",
        "- displays the name and F1 score of the top-performing model\n",
        "\n",
        "#save_dir = f\"./best_model_{best_name}\"  \n",
        "- defines the directory path where the best model will be saved\n",
        "\n",
        "#best_trainer.save_model(save_dir)  \n",
        "- saves the fine-tuned model weights and configuration to the specified directory\n",
        "\n",
        "#tokenizer.save_pretrained(save_dir)  \n",
        "- saves the tokenizer configuration to the same folder for consistent reuse\n",
        "\n",
        "#def predict(texts, model_dir=save_dir):  \n",
        "- defines a helper function to perform predictions on new text samples\n",
        "    #tok = AutoTokenizer.from_pretrained(model_dir)  \n",
        "    - loads the saved tokenizer from the specified model directory\n",
        "    #mdl = AutoModelForSequenceClassification.from_pretrained(model_dir).to(device)  \n",
        "    - loads the saved fine-tuned model and moves it to the computation device\n",
        "    #enc = tok(list(texts), padding=True, truncation=True, max_length=160, return_tensors=\"pt\").to(device)  \n",
        "    - tokenizes the input texts with padding and truncation, returning PyTorch tensors\n",
        "    #with torch.no_grad():  \n",
        "    - disables gradient computation to speed up inference\n",
        "        #logits = mdl(**enc).logits  \n",
        "        - performs a forward pass to get raw prediction scores (logits)\n",
        "    #pred = torch.argmax(logits, dim=-1).cpu().numpy()  \n",
        "    - converts logits to final predicted class labels\n",
        "    #prob = torch.softmax(logits, dim=-1).cpu().numpy()[:,1]  \n",
        "    - converts logits into probabilities and selects the positive-class probability\n",
        "    #return pred, prob  \n",
        "    - returns both the predicted labels and their confidence scores\n",
        "\n",
        "#samples = [  \n",
        "#    \"I feel calm and in control today.\",  \n",
        "#    \"My chest is tight and I cannot focus, I think I am very stressed.\",  \n",
        "#    \"Workload is heavy but manageable so far.\"  \n",
        "#]  \n",
        "- defines example text inputs to test the model‚Äôs predictions\n",
        "\n",
        "#pred, prob = predict(samples)  \n",
        "- runs the prediction helper on the sample texts\n",
        "\n",
        "#for s, y, p in zip(samples, pred, prob):  \n",
        "- iterates over each sample, its predicted label, and probability\n",
        "    #lab = \"stressed(1)\" if y==1 else \"not-stressed(0)\"  \n",
        "    - assigns a readable label name depending on the prediction value\n",
        "    #print(f\"[{lab}  p={p:.3f}]  {s}\")  \n",
        "    - prints the predicted label, probability, and the original sentence\n"
      ],
      "metadata": {
        "id": "SAA1gEu1CTdp"
      },
      "id": "SAA1gEu1CTdp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24E_Tay_hZ2O"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Select the best run from 'results' dict above\n",
        "best_name, best_f1 = None, -1.0\n",
        "best_trainer = None\n",
        "for name,(metrics, trainer) in results.items():\n",
        "    if metrics['eval_f1'] > best_f1:\n",
        "        best_f1 = metrics['eval_f1']\n",
        "        best_name = name\n",
        "        best_trainer = trainer\n",
        "\n",
        "print(f\"Best run: {best_name} with F1={best_f1:.4f}\")\n",
        "\n",
        "# Save the best model for reuse\n",
        "save_dir = f\"./best_model_{best_name}\"\n",
        "best_trainer.save_model(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "# Simple inference helper\n",
        "def predict(texts, model_dir=save_dir):\n",
        "    tok = AutoTokenizer.from_pretrained(model_dir)\n",
        "    mdl = AutoModelForSequenceClassification.from_pretrained(model_dir).to(device)\n",
        "    enc = tok(list(texts), padding=True, truncation=True, max_length=160, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = mdl(**enc).logits\n",
        "    pred = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "    prob = torch.softmax(logits, dim=-1).cpu().numpy()[:,1]\n",
        "    return pred, prob\n",
        "\n",
        "# Demo predictions on a few samples\n",
        "samples = [\n",
        "    \"I feel calm and in control today.\",\n",
        "    \"My chest is tight and I cannot focus, I think I am very stressed.\",\n",
        "    \"Workload is heavy but manageable so far.\"\n",
        "]\n",
        "pred, prob = predict(samples)\n",
        "for s, y, p in zip(samples, pred, prob):\n",
        "    lab = \"stressed(1)\" if y==1 else \"not‚Äëstressed(0)\"\n",
        "    print(f\"[{lab}  p={p:.3f}]  {s}\")\n"
      ],
      "id": "24E_Tay_hZ2O"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "591cdd45c7b74fa181821f9529944da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_776524b1ce5c4c5bb65f8d8e65516273",
              "IPY_MODEL_3d089afa63744558bb3a6f7a1b876aef",
              "IPY_MODEL_e4a09e0a755241daad5e0330cfe68fd2"
            ],
            "layout": "IPY_MODEL_b81c22472d7940c798254fd44d9877b8"
          }
        },
        "776524b1ce5c4c5bb65f8d8e65516273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5da86772c65244f09c694822bd730ce2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1cd05c5690a743bbb2472e7b3d8a91c5",
            "value": "config.json:‚Äá100%"
          }
        },
        "3d089afa63744558bb3a6f7a1b876aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b07464960b8645bbaca506d2ef31bace",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5fde31686a35414f962235ea930bdccb",
            "value": 385
          }
        },
        "e4a09e0a755241daad5e0330cfe68fd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b95ecc756ea41f29095ab7890ab856c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5a1b98fe2c514b45a8153a30ba29225b",
            "value": "‚Äá385/385‚Äá[00:00&lt;00:00,‚Äá33.1kB/s]"
          }
        },
        "b81c22472d7940c798254fd44d9877b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5da86772c65244f09c694822bd730ce2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cd05c5690a743bbb2472e7b3d8a91c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b07464960b8645bbaca506d2ef31bace": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fde31686a35414f962235ea930bdccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b95ecc756ea41f29095ab7890ab856c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a1b98fe2c514b45a8153a30ba29225b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd7b191158e14b56b7fb18d781905220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c86017ab938b4f46baa78b381b03320c",
              "IPY_MODEL_11930ebd9ce2434c8aaf21655a0f564a",
              "IPY_MODEL_e217ae1ba3974278934fbb75fae99d57"
            ],
            "layout": "IPY_MODEL_3c9e5917c4754acbbb908f3431191d45"
          }
        },
        "c86017ab938b4f46baa78b381b03320c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c6926731b6e4c06a63e5a2ef414f6a9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_35c4939c98a141cdaecbbd978211e48d",
            "value": "vocab.txt:‚Äá"
          }
        },
        "11930ebd9ce2434c8aaf21655a0f564a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddd174a933954b4fab9f1640ad491973",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ade9e93734a1456797e0340e38182c04",
            "value": 1
          }
        },
        "e217ae1ba3974278934fbb75fae99d57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74b176f12a3d42139088e6f4dd3f2d12",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_08e6b823fa1042c9892bc49c949a2857",
            "value": "‚Äá213k/?‚Äá[00:00&lt;00:00,‚Äá7.07MB/s]"
          }
        },
        "3c9e5917c4754acbbb908f3431191d45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c6926731b6e4c06a63e5a2ef414f6a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35c4939c98a141cdaecbbd978211e48d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddd174a933954b4fab9f1640ad491973": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ade9e93734a1456797e0340e38182c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74b176f12a3d42139088e6f4dd3f2d12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08e6b823fa1042c9892bc49c949a2857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}